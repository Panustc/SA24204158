---
title: "Introduction to SA24204158"
author: "SA24204158"
date: "2024-11-29"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to SA24204158}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Package Overview

This R package provides implementations of various nearest neighbor-based classifiers, including K-Nearest Neighbors (KNN), Distributed Nearest Neighbor (DNN), and Two-Scale Distributed Nearest Neighbor (TDNN). The package leverages both R and C++ code to optimize the performance of these algorithms, providing a flexible and efficient solution for classification tasks.

## Background Introduction

KNN(K Nearest Neighbor) and WNN(Weighted Nearest Neighbor) classifiers are widely used, but both face certain challenges. For instance, the KNN classifier assigns equal weights of $\frac{1}{k}$ to the $k$ nearest neighbors of $X_i$, despite the intuitive notion that points closer to $X_i$ should exert a greater influence. On the other hand, the WNN classifier struggles with the difficulty of selecting appropriate weights. To address these issues, Steele proposed the Bagged 1-NN Classifier, which integrates the Bagging technique. Steele demonstrated that this approach automatically assigns monotonic, non-negative weights to neighbors across the entire sample distribution, which led us to name this method the Distribution Nearest Neighbor (DNN) in our implementation.

In 2012, Samworth provided risk expressions for both WNN and DNN classifiers under certain regularity conditions. Building upon this, we propose linearly combining two DNN classifiers to reduce the bias of the DNN estimator, resulting in the faster-converging TDNN (Two-Distribution Nearest Neighbors) classifier.


## Key Functions

1.  **`knnC`**:
    -   **Description**: Implements a K-Nearest Neighbors classifier using C++. The function predicts labels for test samples by calculating the distance between each test sample and all training samples, then selecting the majority label from the k-nearest neighbors.
    -   **Usage**: Suitable for simple classification tasks with moderate to large datasets.
2.  **`knnCvC`**:
    -   **Description**: A cross-validation function that selects the optimal value of `k` for the K-Nearest Neighbors classifier. It performs 5-fold cross-validation on the training data to identify the `k` value that maximizes classification accuracy.
    -   **Usage**: Helps in tuning the `k` parameter for KNN classifiers based on cross-validation results.
3.  **`dnnC`**:
    -   **Description**: Implements a Distributed Nearest Neighbor classifier using C++. The classifier computes a weighted average of the distances from the test sample to its nearest neighbors, with weights determined by combinatorial coefficients.
    -   **Usage**: Useful for situations where traditional KNN may not perform optimally, providing an alternative distance-based classification method.
4.  **`dnnCvC`**:
    -   **Description**: A cross-validation function for selecting the optimal subsample size `s` for the DNN classifier. It performs 5-fold cross-validation to find the `s` value that maximizes classification accuracy.
    -   **Usage**: Helps in tuning the subsample size `s` for DNN classifiers based on cross-validation.
5.  **`tdnnC`**:
    -   **Description**: Implements the Two-Scale Distributed Nearest Neighbor (TDNN) classifier, which combines the predictions of two DNN classifiers using weights derived from the subsample sizes `s1` and `s2`.
    -   **Usage**: Used for more complex classification tasks where the two-scale approach provides better performance than traditional methods.
6.  **`tdnnCvC`**:
    -   **Description**: A cross-validation function for selecting the optimal subsample size `s2` for the TDNN classifier, where the ratio of `s1/s2` is fixed to 2. It uses 5-fold cross-validation to find the best `s2` value.
    -   **Usage**: Helps in tuning the `s2` parameter for TDNN classifiers, where `s1` is fixed based on the problem context.
7.  **`benchmark_knn`**:
    -   **Description**: A benchmarking function that compares the efficiency of the KNN algorithm implemented in R and C++. It runs both implementations on the same dataset and measures their runtime to compare performance.
    -   **Usage**: Useful for users who want to assess the speed improvements of using C++ over R for KNN-based classification tasks.

we also add comments for user to better understand:

```{r, eval = FALSE}
library(SA24204158)
help(knnC)
```

------------------------------------------------------------------------

## Example Function Usage

We first show some basic ways to use our functions

### Benchmark KNN Performance

This part demonstrates the necessity of using C++ for developmentï¼š

```{r}
library(Rcpp)
library(SA24204158)
set.seed(123)
# Sample training and test data
train_data <- matrix(c(1, 2, 1, 3, 4, 5, 6, 7), ncol = 2)
train_labels <- c(0, 0, 1, 1)
test_data <- matrix(c(1.5, 2.5, 6, 7), ncol = 2)

# Set the number of neighbors
k <- 3

# Run KNN benchmark
benchmark_results <- benchmark_knn(train_data, train_labels, test_data, k)
print(benchmark_results)
```

On median, C++ is nearly 60 times more efficient than R!

### Predicting with KNN, DNN, and TDNN Classifiers

Next we show how knn, dnn, tdnn are used to solve classification problems:

```{r}
# Sample training and test data
train_data <- matrix(c(1, 2, 1, 3, 4, 5, 6, 7), ncol = 2)
train_labels <- c(0, 0, 1, 1)
test_data <- matrix(c(1.5, 2.5, 6, 7), ncol = 2)

# KNN, DNN, and TDNN predictions
predictions_knn <- knnC(train_data, train_labels, test_data, k = 3)
predictions_dnn <- dnnC(train_data, train_labels, test_data, s = 3)
predictions_tdnn <- tdnnC(train_data, train_labels, test_data, s1 = 1, s2 = 2)

# Print predictions
print(predictions_knn)
print(predictions_dnn)
print(predictions_tdnn)
```

### Cross-Validation to Select Best Parameters

We also provide cross-validation functions to select parameters:

```{r}
set.seed(123)
# Generate synthetic data
train_data <- matrix(rnorm(200 * 2), ncol = 2)
train_labels <- sample(c(0, 1), size = 200, replace = TRUE)

# Cross-validation for KNN (find best k)
k_values <- 1:10
best_k <- knnCvC(train_data, train_labels, k_values)
print(best_k)

# Cross-validation for DNN (find best s)
s_values <- 1:10
best_s <- dnnCvC(train_data, train_labels, s_values)
print(best_s)

# Cross-validation for TDNN (find best s2)
s2_values <- 1:10
best_s2 <- tdnnCvC(train_data, train_labels, s2_values , c=2)
print(best_s2)
```

------------------------------------------------------------------------

#### Explanation of Code:

1.  **Benchmarking KNN**:
    -   This runs the `benchmark_knn` function to compare the performance of the KNN classifier using R and C++ implementations. It uses the training and test datasets along with the specified `k` value.
2.  **Classifier Predictions (KNN, DNN, TDNN)**:
    -   Here, we use the `knnC`, `dnnC`, and `tdnnC` functions to classify the test data based on different classifier methods. You specify `k` for KNN and `s` values for DNN and TDNN classifiers.
3.  **Cross-Validation for Parameter Selection**:
    -   This part demonstrates how to use cross-validation to find the best parameters (`k`, `s`, `s2`) for KNN, DNN, and TDNN classifiers by evaluating their performance over multiple values and selecting the best one.

------------------------------------------------------------------------

### Monte Carlo Experiment Comparing DNN and TDNN

Finally, we set up a model and observe the performance of knn, dnn and tdnn under this model through 100 Monte Carlo experiments.

```{r,echo = FALSE}
rm(list = ls())
```

#### Setting model

Let $f_1$ be the density of $d$ independent components, each having a standard Laplace distribution, and $f_2$ be the density of the $N_d(\theta, I)$ distribution, where $\theta$ denotes a $d$-dimensional vector of ones.

In mathematical notation:

-   $f_1(x) = \prod_{i=1}^{d} \frac{1}{2} \exp(-|x_i|)$, for $x = (x_1, x_2, \dots, x_d) \in \mathbb{R}^d$

-   $f_2(x) = \frac{1}{(2\pi)^{d/2}} \exp\left(-\frac{1}{2}(x - \theta)^T I^{-1} (x - \theta)\right)$, for $x \in \mathbb{R}^d$, where $\theta = (1, 1, \dots, 1)^T$ and $I$ is the identity matrix.

```{r, eval = FALSE}
set.seed(123)
# Set parameters
d <- 10         # Feature dimension
n <- 1000      # Number of data points
n_experiments <- 100  # Number of Monte Carlo experiments

# Load necessary libraries
library(MASS)       # For mvrnorm
library(extraDistr) # For rlaplace

# Function to generate data
generate_data <- function(n, d) {
  # Generate data for class 1 (Laplace distribution)
  f1 <- matrix(rlaplace(n * d, mu = 0, sigma = 1), nrow = n, ncol = d)
  # Generate data for class 2 (Multivariate normal distribution)
  f2 <- mvrnorm(n, mu = rep(1, d), Sigma = diag(d))
  
  # Combine data
  data <- rbind(f1, f2)
  
  # Assign labels: first n samples are class 1, next n samples are class 2
  labels <- c(rep(1, n), rep(0, n))
  
  return(list(data = data, labels = labels))
}

dmvnorm_custom <- function(x, mean, sigma) {
  d <- length(mean)
  diff <- x - mean
  exponent <- -0.5 * t(diff) %*% solve(sigma) %*% diff
  normalization <- 1 / sqrt((2 * pi)^d * det(sigma))
  return(normalization * exp(exponent))
}


# Function to calculate eta(x)
calculate_eta <- function(x) {
  # Probability density for class 1 (Laplace distribution)
  p1 <- exp(-sum(abs(x))) / (2^d)  # Adjust Laplace PDF formula
  
  # Probability density for class 2 (Multivariate normal distribution)
   p2 <- dmvnorm_custom(x, mean = rep(1, d), sigma = diag(d)) # Use custom dmvnorm
  
  # Calculate eta(x)
  eta <- p1 / (p1 + p2)
  
  return(eta)
}

# Function to compute Bayes risk
bayes_risk <- function(data, labels) {
  predicted_class <- apply(data, 1, function(x) {
    eta <- calculate_eta(x)
    ifelse(eta > 0.5, 1, 0)
  })
  
  # Calculate the error rate
  risk <- mean(predicted_class != labels)
  return(risk)
}

# Run Monte Carlo experiments
total_risk <- 0
for (i in 1:n_experiments) {
  # Generate data
  data_info <- generate_data(n, d)
  data <- data_info$data
  labels <- data_info$labels
  
  # Compute Bayes risk
  risk <- bayes_risk(data, labels)
  
  # Accumulate the risk
  total_risk <- total_risk + risk
}

# Calculate and print the average risk
average_risk <- total_risk / n_experiments
print(paste("Average Bayes Risk:", average_risk))

```

With the above code, we can calculate the Bayesian risk of different d models.

| d   | 1    | 2 | 3 | 5 | 10 |
|-----|------|------|------|------|------|------|
| Bayes Risk|30.06| 24.38| 19.40| 13.15|5.618 |


```{r,echo = FALSE}
rm(list = ls()) 
```

Then, we use **Monte Carlo method** to estimate the risk of dnn and tdnn:

```{r, eval = FALSE}
library(Rcpp)
library(SA24204158)
library(MASS)
library(extraDistr)
set.seed(123)
# Function to run a single experiment and compute accuracy and time
run_experiment <- function(d, n_train, n_test) {
  # Record start time
  start_time <- Sys.time()

  # Generate training and test sets
  train_f1 <- matrix(rlaplace(n_train * d, mu = 0, sigma = 1), nrow = n_train, ncol = d)
  train_f2 <- mvrnorm(n_train, mu = rep(1, d), Sigma = diag(d))
  test_f1 <- matrix(rlaplace(n_test * d, mu = 0, sigma = 1), nrow = n_test, ncol = d)
  test_f2 <- mvrnorm(n_test, mu = rep(1, d), Sigma = diag(d))

  # Combine training and test sets
  train_data <- rbind(train_f1, train_f2) 
  test_data <- rbind(test_f1, test_f2)
  
  # Assign labels: first n_train samples are class 0, next n_train samples are class 1
  train_labels <- c(rep(0, n_train), rep(1, n_train))
  test_labels <- c(rep(0, n_test), rep(1, n_test))
  
  # Use CV to select parameters for DNN and TDNN
  s_values <- 1:10
  best_s <- floor((5/4)**(d/(d+4))*dnnCvC(train_data, train_labels, s_values))
  s2_values <- 1:10
  best_s2 <- floor((5/4)**(d/(d+8))*tdnnCvC(train_data, train_labels, s2_values , c=2))
  
  # Predict using DNN
  dnn_predictions <- dnnC(train_data, train_labels, test_data, best_s)
  
  # Predict using TDNN
  s1 <- 2 * best_s2
  tdnn_predictions <- tdnnC(train_data, train_labels, test_data, s1, best_s2)
  
  # Compute accuracies
  dnn_accuracy <- mean(dnn_predictions == test_labels)
  tdnn_accuracy <- mean(tdnn_predictions == test_labels)

  # Record end time and compute elapsed time
  end_time <- Sys.time()
  elapsed_time <- as.numeric(difftime(end_time, start_time, units = "secs"))

  return(list(dnn_accuracy = dnn_accuracy, tdnn_accuracy = tdnn_accuracy, elapsed_time = elapsed_time))
}

# Set experiment parameters
d <- 1  # Feature dimension
n_train <- 1000   #Number of training samples
n_test <- 200   # Number of test samples
n_iterations <- 100  # Number of Monte Carlo experiments

# Store accuracies and times for each experiment
dnn_accuracies <- numeric(n_iterations)
tdnn_accuracies <- numeric(n_iterations)
times <- numeric(n_iterations)

# Run Monte Carlo experiments
for (i in 1:n_iterations) {
  results <- run_experiment(d, n_train, n_test)
  dnn_accuracies[i] <- results$dnn_accuracy
  tdnn_accuracies[i] <- results$tdnn_accuracy
  times[i] <- results$elapsed_time
}

# Compute average accuracies
mean_dnn_accuracy <- mean(dnn_accuracies)
mean_tdnn_accuracy <- mean(tdnn_accuracies)

# Compute average elapsed time
mean_time <- mean(times)

# Output results
cat("Average DNN Accuracy: ", mean_dnn_accuracy, "\n")
cat("Average TDNN Accuracy: ", mean_tdnn_accuracy, "\n")
cat("Average DNN Risk: ", 1-mean_dnn_accuracy, "\n")
cat("Average TDNN Risk: ", 1-mean_tdnn_accuracy, "\n")
cat("Average Time per Experiment: ", mean_time, "seconds\n")
```

| d   | n    | Bayes Risk | Dnn Risk | Tdnn Risk |
|-----|------|------------|----------|-----------|
| 1   | 1000 | 30.06      | 30.46    | 30.48     |
| 2   | 1000 | 24.38      | 25.94    | 25.85     |
| 3   | 1000 | 19.40      | 21.77    | 21.27     |
| 5   | 1000 | 13.15      | 17.47    | 16.12     |
| 10  | 1000 | 5.618      | 12.70    | 9.273     |

Take d=5 for example, we can see that TDNN improves the accuracy by approximately 1.5% over DNN. By comparing $\frac{Risk(\text{TDNN}) - Risk(\text{Bayes})}{Risk(\text{DNN}) - Risk(\text{Bayes})}$, we find that the accuracy improvement of TDNN is around 35%.


## References

1.  Samworth, Richard J. "Optimal weighted nearest neighbour classifiers." *Ann. Statist.* 40, no. 5 (2012): 2733â€“2763.

2.  Steele, Brian M. "Exact bootstrap k-nearest neighbor learners." *Mach. Learn.* 74, no. 3 (2009): 235â€“255. Springer.

3.  Demirkaya, Emre, Fan, Yingying, Gao, Lan, Lv, Jinchi, Vossler, Patrick, and Wang, Jingbo. "Optimal nonparametric inference with two-scale distributional nearest neighbors." *J. Amer. Statist. Assoc.* 119, no. 545 (2024): 297â€“307. Taylor & Francis.
