---
title: "Homework Organization"
author: "SA24204158"
date: "2024-11-29"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework Analysis}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

## 文档简介

这个文档是2024秋季USTC统计算法课程作业的整理，包含**Homework1-Homework9**。

### Homework-1

#### 任务说明

本次作业让我们用R markdown文件展示三个图文(图表)结合的例子

##### 例一

例一我们以R语言自带的数据集"mtcars"为例，生成了一副**箱线图**（boxplot），展示了不同气缸数量（`cyl`）对应的每加仑行驶英里数（`mpg`）的分布情况。

```{r}
library(ggplot2)
### 数据集预览
mtcars
summary(mtcars)
```

```{r}
### 生成箱线图
ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +
  geom_boxplot() +
  labs(title = "Miles per Gallon by Number of Cylinders")
```

从上述箱线图可以看出，气缸数量（`cyl`）越高的车，每加仑油行驶英里数（`mpg`）更少，即每公里耗油量更大。

##### 例二

R markdown支持展示latex表格和公式，使用起来十分方便。

```{r}
xtable::xtable(head(iris))
```

```{r, message=FALSE, warning=FALSE}
# 加载knitr包
library(knitr)

# 创建数据框
data <- data.frame(
  Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5.0, 5.4),
  Sepal.Width = c(3.5, 3.0, 3.2, 3.1, 3.6, 3.9),
  Petal.Length = c(1.4, 1.4, 1.3, 1.5, 1.4, 1.7),
  Petal.Width = c(0.2, 0.2, 0.2, 0.2, 0.2, 0.4),
  Species = c("setosa", "setosa", "setosa", "setosa", "setosa", "setosa")
)

# 使用kable生成表格
kable(data, caption = "Iris数据集的部分内容")
```

下面展示一些统计中常见的公式，包括均值、方差、全概率公式以及贝叶斯公式。

###### 均值公式：

$$ \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i $$

###### 方差公式：

$$ \sigma^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2 $$

###### 全概率公式：

$$ P(A) = \sum_{i} P(A \mid B_i) P(B_i) $$

###### 贝叶斯公式：

$$ P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$

###### 条件熵公式：

$$ H(Y|X) = \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log \left( \frac{p(x)}{p(x, y)} \right)$$
这个条件熵公式也是老师ppt上的例子。

##### 例三

在这个例子我们展示R语言适用于统计的便利性。

R语言可以生成多种服从不同分布的样本，我们以标准正态分布为例：

$$f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$$

```{r}
# 设置图形布局为1行2列
par(mfrow = c(1, 2))  

# 生成100个标准正态分布的随机点
set.seed(123)
data_100 <- rnorm(100)

# 绘制100个样本点的直方图
hist(data_100, breaks = 10, probability = TRUE, 
     main = "标准正态分布的直方图\n(样本点数 = 100)", 
     xlab = "X", 
     col = "lightblue")
curve(dnorm(x), col = "red", lwd = 2, add = TRUE)

# 生成1000个标准正态分布的随机点
data_1000 <- rnorm(1000)

# 绘制1000个样本点的直方图
hist(data_1000, breaks = 20, probability = TRUE, 
     main = "标准正态分布的直方图\n(样本点数 = 1000)", 
     xlab = "X", 
     col = "lightblue")
curve(dnorm(x), col = "red", lwd = 2, add = TRUE)

# 恢复默认的图形布局
par(mfrow = c(1, 1))


```

可以看到，随着样本点个数n的增大，生成的随机样本分布与给定的分布越来越近似。

除此之外，R语言强大的统计工具不仅限于生成随机样本，还可以进行多种统计分析，如回归分析、假设检验、方差分析等。

1.  **线性回归**：R语言可以通过`lm()`函数进行线性回归分析，并轻松提取回归系数和模型诊断信息。

2.  **假设检验**：如`t检验`、`卡方检验`、`F检验`等，R语言提供了丰富的检验函数，如`t.test()`、`chisq.test()`等。

3.  **方差分析（ANOVA）**：R语言可以通过`aov()`函数进行方差分析，用于比较多个组之间的均值差异是否显著。

###### 线性回归示例

我们可以通过一组模拟数据来展示线性回归分析的功能：

```{r}
# 生成模拟数据
set.seed(123)
x <- rnorm(100)
y <- 2*x + rnorm(100)

# 线性回归模型
model <- lm(y ~ x)

# 输出回归结果
summary(model)

# 绘制回归图
plot(x, y, main = "线性回归图", col = "blue")
abline(model, col = "red", lwd = 2)
```

从回归结果中，我们可以轻松得到回归系数、残差分析等信息，并且可以通过图形展示回归结果与数据的拟合情况。

#### 总结

我们展示了R markdown的强大功能与便利性。R语言作为统计中最常用的语言，R
markdown为R语言创造了一个良好的表示环境，使数据分析、可视化和报告生成可以在一个统一的框架中无缝集成，这使得其成为现代统计分析和数据科学的重要工具。

### Homework-2

#### 练习一 ：Rayleigh分布随机数生成

Rayleigh分布的概率密度函数如下：

$$
f(x) = \frac{x}{\sigma^2} e^{-x^2/(2\sigma^2)}, \quad x \geq 0, \, \sigma > 0.
$$

写出一个算法来生成服从Rayleigh$(\sigma)$分布的随机样本，并通过直方图来验证。

这里我采用的是逆变换法，因为Rayleigh分布的累计分布函数比较好求出：

$$
F(x) = 1 - \exp\left(-\frac{x^2}{2\sigma^2}\right)
$$

其中，$\sigma$ 是尺度参数，$x \geq 0$。

```{r}
set.seed(123)

# 设定参数
sigma_values <- c(1,3,5,7)  # 不同的尺度参数
n <- 10000  # 生成随机数的数量

# 循环遍历每个sigma值
for (sigma in sigma_values) {
  # 生成均匀分布的随机数
  U <- runif(n)
  
  # 使用逆变换法生成Rayleigh分布的随机数
  X <- sigma * sqrt(-2 * log(1-U))
  
  # 绘制直方图
  hist(X, breaks=30, probability=TRUE,
       main=paste("Rayleigh Distribution (sigma =", sigma, ")"),
       xlab="Value", ylab="Density", xlim=c(0, 15))
  
  # 添加理论密度曲线
  curve((x/(sigma^2)) * exp(-x^2/(2*sigma^2)), add=TRUE, col="blue", lwd=2)
}

```

在这里，我选取了逆变换法生成随机数，并展示了样本数为10000，$\sigma$=(1,3,5,7)时的直方图(密度曲线)。可以看到，由逆变换法生成Rayleigh分布随机数较好的服从了Rayleigh分布。

#### 练习二：正态位置混合分布随机样本生成

生成大小为1000的正态混合分布随机样本。该混合的组成部分分别为 $N(0, 1)$
和 $N(3, 1)$ 分布，它们的混合概率分别为 $p_1$ 和
$p_2 = 1 - p_1$。绘制样本的直方图，并叠加密度曲线，首先选择
$p_1 = 0.75$。随后，用不同的 $p_1$
值重复实验，并观察经验分布是否呈现双峰。对于会产生双峰混合分布的 $p_1$
值给出预测。

首先绘制$p_1$=0.75时的图像：

```{r}
N <- 1000 # 样本容量
set.seed(123)
p1 = 0.75
p <- c(p1, 1-p1)

# 生成混合分布
k <- sample(0:1, size = N, replace = TRUE, prob = p)
y1 <- rnorm(N)
y2 <- rnorm(N, mean = 3, sd = 1)
x <- (1 - k) * y1 + k * y2

# 绘制直方图和核密度估计曲线
hist(x, probability = TRUE, main = paste("p1 =", p1), col = 4, breaks = 100)
lines(density(x), lwd = 3, col = 2)
```

可以看到$p_1$取0.75时，双峰已经不太明显了。

选择不同的 $p_1$ 值重复实验：

```{r}
N <- 1000  # 样本容量
set.seed(123)

# 定义不同的p1值
p1_vals <- seq(from = 0.1, to = 0.9, by = 0.1)

# 生成图形
for (p1 in p1_vals) {
  p <- c(p1, 1 - p1)
  k <- sample(0:1, size = N, replace = TRUE, prob = p)
  
  # 生成两个正态分布
  y1 <- rnorm(N)
  y2 <- rnorm(N, mean = 3, sd = 1)
  
  # 混合分布
  x <- (1 - k) * y1 + k * y2
  
  # 绘制直方图和核密度估计曲线
  hist(x, probability = TRUE, main = paste("p1 =", p1), col = 4, breaks = 50, border = "white", xlab = "", ylab = "")
  lines(density(x), lwd = 2, col = 2)
}


```

从不同得到$p$的表现可以推测，当$p_1\in[0.3,07]$时，混合分布呈现较明显的双峰。

**一些小问题**：观察$p_1=0.75$时的直方图，可以发现其直方图与真实密度曲线拟合的并不是十分完美，这应该是样本数量较小导致的，在样本量增加到10000时会发现拟合的很好，所以在练习一和练习三中，我都设置的生成样本量为10000。

#### 练习三：复合泊松过程模拟

复合泊松过程是一个随机过程 $\{X(t), t \geq 0\}$，可以表示为随机和： $$
X(t) = \sum_{i=1}^{N(t)} Y_i, \quad t \geq 0,
$$ 其中 $\{N(t), t \geq 0\}$ 是一个泊松过程，且 $Y_1, Y_2, \dots$
是独立同分布的随机变量，并且独立于 $\{N(t), t \geq 0\}$。

编写程序模拟一个复合泊松$(\lambda)$-Gamma过程（其中 $Y$
服从Gamma分布）。估计 $X(10)$
的均值和方差，针对多个参数组合进行模拟，并与理论值进行比较。

在这里我给出**两种**不同模拟 $N(t)$ 的方法：

#### 练习三 法一

```{r}
set.seed(123)  # 设置随机种子保证结果可重复

# 参数设置
lambda <- 4    # 泊松过程的λ参数
shape <- 5     # Gamma分布的形状参数 (shape)
scale <- 6     # Gamma分布的尺度参数 (scale)
t <- 10        # 时间t
n_sim <- 10000 # 模拟次数

# 到达间隔时间随速率λ呈指数分布
pp.exp <- function(t0) {
  Tn <- rexp(1000, lambda)  # 生成间隔时间的指数分布
  Sn <- cumsum(Tn)          # 累积和计算事件发生的时间
  return(min(which(Sn > t0)) - 1)  # 返回第一个超过时间t的索引
}

# 生成服从泊松分布的N(t)
N_t <- replicate(n_sim, pp.exp(t))

# 生成复合泊松过程的X(t)
X_t <- sapply(N_t, function(n) {
  Y <- rgamma(n = n, shape = shape, scale = scale)  # Gamma分布
  sum(Y[1:n])  # 求和得到X(t)
})

# 估计均值和方差
mean_X_t <- mean(X_t)
var_X_t <- var(X_t)

# 理论均值和方差
theoretical_mean <- lambda * t * shape * scale
theoretical_var <- lambda * t * (shape + 1) * shape * scale^2

# 输出结果
matrix(c(mean_X_t, theoretical_mean, var_X_t, theoretical_var), 
       ncol = 4, 
       dimnames = list(c("value"), c("Estimated Mean", "Theoretical Mean", "Estimated Variance", "Theoretical Variance")))


```

#### 练习三 法二

```{r}
set.seed(123)  # 设置随机种子保证结果可重复

# 参数设置
lambda <- 4    # 泊松过程的λ参数
t <- 10        # 时间t
shape <- 5     # Gamma分布的形状参数 (shape)
scale <- 6     # Gamma分布的尺度参数 (scale)
n_sim <- 10000 # 模拟次数

# 模拟复合泊松过程
X_t <- numeric(n_sim)  # 存储X(t)的结果

for (i in 1:n_sim) {
  N_t <- rpois(1, lambda * t)  # 生成N(t)，泊松分布
  if (N_t > 0) {
    Y <- rgamma(N_t, shape=shape, scale=scale)  # 生成Y_i，Gamma分布 (使用shape和scale)
    X_t[i] <- sum(Y)  # 求和得到X(t)
  } else {
    X_t[i] <- 0  # 当N(t)=0时，X(t)=0
  }
}

# 估计均值和方差
mean_X_t <- mean(X_t)
var_X_t <- var(X_t)

# 理论均值和方差
theoretical_mean <- lambda * t * shape * scale
theoretical_var <- lambda * t * shape*(shape+1) * (scale^2)

# 输出结果
matrix(c(mean_X_t,theoretical_mean,var_X_t,theoretical_var),ncol = 4,
          dimnames = list(c("value"),c("Estimated Mean","Theoretical Mean","Estimated Variance", "Theoretical Variance")))
```

无论是**法一**与**法二**，二者给出的拟合结果都很好，很接近真实值。

**法一**与**法二**的主要区别在于泊松过程 $N(t)$ 的生成方式：

1.  **法一**：
    -   通过生成指数分布的间隔时间来模拟泊松过程。这种方法更接近于泊松过程的定义，即时间间隔服从指数分布，然后通过累积和来确定事件发生的数量。
    -   使用了 `rexp()` 函数生成事件间隔，并用累积和找到超过 $t$
        的最小时间点。这是一种基于间隔时间的模拟。
2.  **法二**：
    -   直接使用 `rpois()` 函数生成在时间 $t$
        内的事件数量。这是通过泊松分布的性质直接生成事件数量的一种方法。
    -   `rpois(1, lambda * t)` 直接根据泊松过程的公式生成在给定时间 $t$
        内的事件总数。

-   **法一**更适用于需要基于间隔时间来研究事件发生的时间点的情形，它模拟了泊松过程的“本质”——间隔时间服从指数分布。
-   **法二**使用了更简便的 `rpois()`
    函数，直接生成了事件数量，是一种更加高效的实现方法，适合快速生成泊松事件的场合。

两种方法在估计复合泊松过程的 $X(t)$
的均值和方差时，得出了较为接近的结果，但法二的实现更加简洁，适合大规模模拟。

### Homework-3

#### 练习一：

**5.4 编写一个函数来计算 Beta(3, 3)
分布的CDF的蒙特卡罗估计，并使用该函数估计 F(x)，其中 x = 0.1, 0.2, ...,
0.9。将估计值与 R 中 `pbeta` 函数的返回值进行比较。**

```{r}
# 设置参数
alpha <- 3
beta <- 3

# Monte Carlo方法估计Beta分布的CDF
mc_beta_cdf <- function(x, n = 10000) {
  # 从Beta(3, 3)分布中生成n个随机样本
  samples <- rbeta(n, alpha, beta)
  # 计算P(X <= x)的比例
  return(mean(samples <= x))
}

# 估计不同x值下的CDF
x_values <- seq(0.1, 0.9, by = 0.1)
mc_estimates <- sapply(x_values, mc_beta_cdf)

# 使用pbeta函数计算真实值
pbeta_values <- pbeta(x_values, alpha, beta)

# 打印结果
results <- data.frame(x = x_values, MonteCarlo_Estimate = mc_estimates, pbeta_Value = pbeta_values)
print(results)

# 比较两个结果
plot(x_values, mc_estimates, type = "b", col = "blue", pch = 19, ylim = range(c(mc_estimates, pbeta_values)),
     xlab = "x", ylab = "F(x)", main = "Comparison of Monte Carlo and pbeta")
lines(x_values, pbeta_values, type = "b", col = "red", pch = 17)
legend("topleft", legend = c("Monte Carlo Estimate", "pbeta Value"), col = c("blue", "red"), pch = c(19, 17))
```

从结果可以看出，用蒙特卡罗估计得到的值与真实值十分接近。

#### 练习二

**5.9 Rayleigh 分布的概率密度函数为：**

$$
f(x) = \frac{x}{\sigma^2} e^{-x^2 / (2\sigma^2)}, \quad x \geq 0, \sigma > 0.
$$

**实现一个函数，使用对偶变量（antithetic variables）从 Rayleigh(σ)
分布中生成样本。比较**

$$
\frac{X + X'}{2}
$$

**的方差相对于独立的** $X_1, X_2$ 的方差减少了多少百分比？

```{r}
# 设置Rayleigh分布的参数
sigma <- 1
n <- 10000  # 样本数量

# 定义生成Rayleigh分布的函数，这里采用逆变换法
rayleigh_sample <- function(n, sigma) {
  U <- runif(n)
  X <- sigma * sqrt(-2 * log(U))
  return(X)
}

# 生成对偶变量样本
rayleigh_antithetic <- function(n, sigma) {
  U <- runif(n/2)
  X1 <- sigma * sqrt(-2 * log(U))
  X2 <- sigma * sqrt(-2 * log(1 - U))
  return(c(X1, X2))
}

# 计算独立样本的均值和方差
X_ind <- rayleigh_sample(n, sigma)
mean_ind <- mean(X_ind)
var_ind <- var(X_ind)

# 计算对偶变量样本的均值和方差
X_antithetic <- rayleigh_antithetic(n, sigma)
X_combined <- (X_antithetic[1:(n/2)] + X_antithetic[(n/2 + 1):n]) / 2
mean_combined <- mean(X_combined)
var_combined <- var(X_combined)

# 计算方差减少百分比
percent_reduction <- (var_ind - var_combined) / var_ind * 100

# 打印结果
cat("独立样本的均值:", mean_ind, "\n")
cat("对偶变量样本的均值:", mean_combined, "\n")
cat("独立样本的方差:", var_ind, "\n")
cat("对偶变量样本的方差:", var_combined, "\n")
cat("方差减少百分比:", percent_reduction, "%\n")

```

可以发现，采用对偶变量法可有效缩减方差。

#### 练习三

**5.13：找到两个重要性函数** $f_1$ 和 $f_2$，它们定义在区间
$(1, \infty)$ 上，并且与

$$
g(x) = \frac{x^2}{\sqrt{2\pi}} e^{-x^2 / 2}, \quad x > 1
$$

**接近。在使用重要性采样估计**

$$
\int_1^{\infty} \frac{x^2}{\sqrt{2\pi}} e^{-x^2 / 2} \, dx
$$

**的过程中，哪个重要性函数应该产生较小的方差？并进行解释。**

##### 解答过程

首先我们用数值积分计算积分真实值

```{r}
f <- function(x) {
  (x^2 / sqrt(2 * pi)) * exp(-x^2 / 2)
}

result <- integrate(f, lower = 1, upper = Inf)

cat("积分结果: ", result$value, "\n")

```

为了对比明显，我选择了三个重要性函数，分别是：

$$
g(x) = \frac{x^2}{\sqrt{2\pi}} e^{-x^2 / 2}
$$

1.  截断的正态分布 $f_1(x)$： $$
    f_1(x) = \frac{\phi(x)}{1 - \Phi(1)}, \quad x > 1
    $$ 其中，$\phi(x)$ 为标准正态分布的概率密度函数，$\Phi(x)$
    为标准正态分布的累积分布函数。

2.  指数分布 $f_2(x)$，$\lambda = 1$： $$
    f_2(x) = e^{-(x - 1)}, \quad x > 1
    $$

3.  Rayleigh 分布 $f_3(x)$，$\sigma = 1$： $$
    f_3(x) = \frac{x \cdot e^{-x^2 / 2}}{1 - \left(1 - e^{-1/2}\right)}, \quad x > 1
    $$

```{r, eval = FALSE}
set.seed(123)
# 定义 g(x) 函数
g <- function(x) {
  (x^2 / sqrt(2 * pi)) * exp(-x^2 / 2)
}

# 截断的正态分布 (f_1) 密度函数
f1 <- function(x) {
  dnorm(x) / (1 - pnorm(1))  # 截断的标准正态分布（x > 1）
}

# 指数分布 (f_2) 密度函数，lambda = 1
f2 <- function(x) {
  exp(-(x - 1))  # 指数分布的 PDF (x > 1)
}

# Rayleigh 分布 (f_3) 密度函数，sigma = 1
f3 <- function(x) {
  ifelse(x > 1, (x * exp(-x^2 / 2)) / (1 - (1 - exp(-1/2))), 0)  # 归一化的 PDF
}

# 生成 f_1 和 f_2 下的样本
sample_f1 <- function(n) {
  # 初始化一个空的向量，用来存储大于1的随机数
  result <- c()
  
  # 继续生成随机数，直到我们收集到n个大于1的数
  while (length(result) < n) {
    # 从标准正态分布中生成随机数
    random_numbers <- rnorm(n)
    
    # 选择大于1的随机数
    valid_numbers <- random_numbers[random_numbers > 1]
    
    # 将这些大于1的数添加到结果中
    result <- c(result, valid_numbers)
  }
  
  # 返回前n个大于1的数
  return(result[1:n])
}

sample_f2 <- function(n) {
  result <- c()
  while (length(result) < n) {
    random_numbers <- rexp(n)
    valid_numbers <- random_numbers[random_numbers > 1]
    result <- c(result, valid_numbers)
  }
  return(result[1:n])
}

# 从 Rayleigh 分布中生成样本，确保归一化
sample_f3 <- function(n) {
  result <- c()
  
  # 继续生成随机数，直到我们收集到n个大于1的数
  while (length(result) < n) {
    # 生成 Rayleigh 分布的随机数，使用 sqrt(-2 * log(U))
    random_numbers <- sqrt(-2 * log(runif(n)))
    valid_numbers <- random_numbers[random_numbers > 1]
    result <- c(result, valid_numbers)
  }
  
  # 返回前n个大于1的数
  return(result[1:n])
}

# 重要性采样估计积分
importance_sampling <- function(f_sample, f_density, g, n) {
  x <- f_sample(n)
  weights <- g(x) / f_density(x)
  return(mean(weights))
}

# 设置样本数
n <- 10000

# 使用 f_1 进行重要性采样
estimate_f1 <- importance_sampling(sample_f1, f1, g, n)
cat("使用 f_1 的重要性采样估计值:", estimate_f1, "\n")

# 使用 f_2 进行重要性采样
estimate_f2 <- importance_sampling(sample_f2, f2, g, n)
cat("使用 f_2 的重要性采样估计值:", estimate_f2, "\n")

# 使用 f_3 进行重要性采样
estimate_f3 <- importance_sampling(sample_f3, f3, g, n)
cat("使用 f_3 的重要性采样估计值:", estimate_f3, "\n")

# 计算方差比较
samples_f1 <- sample_f1(n)
samples_f2 <- sample_f2(n)
samples_f3 <- sample_f3(n)

variance_f1 <- var(g(samples_f1) / f1(samples_f1))
variance_f2 <- var(g(samples_f2) / f2(samples_f2))
variance_f3 <- var(g(samples_f3) / f3(samples_f3))

cat("f_1 的重要性采样方差:", variance_f1, "\n")
cat("f_2 的重要性采样方差:", variance_f2, "\n")
cat("f_3 的重要性采样方差:", variance_f3, "\n")

# 计算方差减少百分比
percent_reduction_f1_f2 <- (variance_f1 - variance_f2) / variance_f1 * 100
cat("f_2 相对于 f_1 的方差减少百分比:", percent_reduction_f1_f2, "%\n")

percent_reduction_f1_f3 <- (variance_f1 - variance_f3) / variance_f3 * 100
cat("f_3 相对于 f_1 的方差减少百分比:", percent_reduction_f1_f3, "%\n")

```

对比结果我们发现方差表现：$f_3>f_2>f_1$，其实这有点违反直觉，因为直觉上，我们感觉$e^{-x^2}$相比于$e^{-x}$与$x^2e^{-x^2}$更接近(见下图)，但$f_2$表现优于$f_1$。

```{r}
# 设置图形设备
set.seed(123)
x <- seq(0, 5, length.out = 1000)

# 定义 g(x) 函数
g <- function(x) {
  (x^2 / sqrt(2 * pi)) * exp(-x^2 / 2)
}

# 计算各个分布的概率密度函数
g_values <- g(x)
normal_values <- dnorm(x)  # 正态分布
exponential_values <- dexp(x - 1)  # 指数分布，偏移至 x > 1
rayleigh_values <- ifelse(x > 0, (x / 1) * exp(-x^2 / 2), 0)  # Rayleigh 分布，sigma = 1

# 绘图
plot(x, g_values, type = "l", col = "blue", lwd = 3,xlim = c(1, 5), ylim = c(0, 1),
     xlab = "x", ylab = "Density", main = "Comparison of g(x) and Various Distributions")
lines(x, normal_values, col = "orange", lwd = 3)
lines(x, exponential_values, col = "black", lwd = 3)
lines(x, rayleigh_values, col = "red", lwd = 3)
legend("topright", legend = c("g(x)", "Normal Distribution (f1)", "Exponential Distribution (f2)", "Rayleigh Distribution (f3)"),
       col = c("blue", "orange", "black", "red"), lty = 1, lwd = 3)
grid()

```

在发现这个问题后，我去翻阅了课本，发现我们实际应比较的是$g(x)/f(x)$，应选择使这个比值更接近常数的f(x)，做出$g(x)/f_i(x)$，$i=1,2,3$的图后我们可以发现：

```{r}
g <- function(x) {
  (x^2 / sqrt(2 * pi)) * exp(-x^2 / 2)
}

# 截断的正态分布 (f_1) 密度函数
f1 <- function(x) {
  dnorm(x) / (1 - pnorm(1))  # 截断的标准正态分布（x > 1）
}

# 指数分布 (f_2) 密度函数，lambda = 1
f2 <- function(x) {
  exp(-(x - 1))  # 指数分布的 PDF (x > 1)
}

# Rayleigh 分布 (f_3) 密度函数，sigma = 1
f3 <- function(x) {
(x * exp(-x^2 / 2)) / (1 - exp(-1/2))  # 归一化的 Rayleigh 分布
}

# 设置 x 的取值范围
x_values <- seq(1, 6, length.out = 1000)

# 计算 g(x) / f1(x), g(x) / f2(x), g(x) / f3(x)
y_f1 <- g(x_values) / f1(x_values)
y_f2 <- g(x_values) / f2(x_values)
y_f3 <- g(x_values) / f3(x_values)

# 绘图
plot(x_values, y_f1, type = "l", col = "blue", lwd = 2,xlim = c(1,5),ylim = c(0, max(c(y_f1, y_f2, y_f3))),
     xlab = "x", ylab = "g(x) / f(x)", main = "g(x) / f(x) for f1, f2, f3 Distributions")
lines(x_values, y_f2, col = "green", lwd = 2)
lines(x_values, y_f3, col = "red", lwd = 2)

# 添加图例
legend("topright", legend = c("Normal Distribution g(x) / f1(x)", " Exponential Distribution g(x) / f2(x)", " Rayleigh Distribution g(x) / f3(x)"),
       col = c("blue", "green", "red"), lwd = 2)

```

接近常数的程度：$g(x)/f_3(x)>g(x)/f_2(x)>g(x)/f_1(x)$，所以表现：$f_3>f_2>f_1$。

#### ppt上练习

**Monte Carlo 实验**

**对于**
$n = 10^4, 2 \times 10^4, 4 \times 10^4, 6 \times 10^4, 8 \times 10^4$，对随机排列的数字
$1, \dots, n$ 应用快速排序算法。\
计算 100 次模拟的平均计算时间，记为 $a_n$。\
对 $t_n := n \log(n)$ 回归 $a_n$，并以图形显示结果（散点图和回归线）。

```{r, eval = FALSE}
set.seed(123)

# 定义要测试的n值
n_values <- c(10^4, 2 * 10^4, 4 * 10^4, 6 * 10^4, 8 * 10^4)

# 存储每个n的平均计算时间
avg_times <- numeric(length(n_values))

# 进行模拟，测试每个n的排序时间
for (i in seq_along(n_values)) {
  n <- n_values[i]
  times <- numeric(100)  # 存储100次模拟的时间
  
  # 对每个n进行100次排序模拟
  for (j in 1:100) {
    vec <- sample(1:n, n)  # 生成1到n的随机排列
    times[j] <- system.time(sort(vec, method = "quick"))[3]  # 记录排序时间
  }
  
  avg_times[i] <- mean(times)  # 计算平均时间
}

# 计算 t_n = n * log(n)
log_n_values <- n_values * log(n_values)

# 回归分析: avg_times 作为因变量，log_n_values 作为自变量
regression <- lm(avg_times ~ log_n_values)

# 打印回归结果
summary(regression)

# 绘图：散点图和回归线
plot(log_n_values, avg_times, main = "Sorting Time vs n * log(n)",
     xlab = "n * log(n)", ylab = "Average Sorting Time (seconds)", pch = 16, col = "blue")
abline(regression, col = "red", lwd = 2)  # 添加回归线

```

从结果来看，如果直接采用sort()函数中自带的快速排序法，得到的拟合并不能直观的看出是线性的，于是我决定改为自己编写快速排序函数，结果如下：

```{r, eval = FALSE}
set.seed(123)

# 定义要测试的n值
n_values <- c(10^4, 2 * 10^4, 4 * 10^4, 6 * 10^4, 8 * 10^4)

# 存储每个n的平均计算时间
avg_times <- numeric(length(n_values))

# 定义快速排序算法
quick_sort <- function(vec) {
  if (length(vec) <= 1) {
    return(vec)  # 如果只有一个元素或空，直接返回
  } else {
    pivot <- vec[1]  # 选择第一个元素作为主元
    left <- quick_sort(vec[vec < pivot])  # 小于主元的部分递归排序
    right <- quick_sort(vec[vec > pivot])  # 大于主元的部分递归排序
    return(c(left, vec[vec == pivot], right))  # 连接结果
  }
}

# 进行模拟，测试每个n的排序时间
for (i in seq_along(n_values)) {
  n <- n_values[i]
  times <- numeric(100)  # 存储100次模拟的时间
  
  # 对每个n进行100次排序模拟
  for (j in 1:100) {
    vec <- sample(1:n, n)  # 生成1到n的随机排列
    times[j] <- system.time(quick_sort(vec))[3]  # 使用手写快速排序，并记录排序时间
  }
  
  avg_times[i] <- mean(times)  # 计算平均时间
}

# 计算 t_n = n * log(n)
log_n_values <- n_values * log(n_values)

# 回归分析: avg_times 作为因变量，log_n_values 作为自变量
regression <- lm(avg_times ~ log_n_values)

# 打印回归结果
summary(regression)

# 绘图：散点图和回归线
plot(log_n_values, avg_times, main = "Sorting Time vs n * log(n)",
     xlab = "n * log(n)", ylab = "Average Sorting Time (seconds)", pch = 16, col = "blue")
abline(regression, col = "red", lwd = 2)  # 添加回归线

```

从这个结果就能看出很好的线性关系，推测可能是sort()函数中的快速排序法在运算复杂度方面进行了某些优化（从运算时间上可以看出sort()函数中的快速排序法比自己编写的快速排序函数运行时间更快），采用自己编写的快速排序函数就能很好的看出快排的平均时间复杂度是$O(nlogn)$。

### Homework-4

#### 作业说明

在课堂上，老师对这次作业提出了一些要求，具体如下：

**复杂模拟研究的建议：**\
- 在复杂的模拟研究中，至少使用三个独立的函数： - **数据生成**； -
**统计推断（准备用于真实数据分析）**； - **结果报告**。

**作业评分标准（针对助教）：** - 检查是否有至少三个独立的函数（否则扣 1
分）。 - 检查是否在调用每个函数之前清理了内存（否则再扣 1 分）。

在这里，我采用

```{r}
rm(list = ls())
```

来清除所有变量。

#### 练习6.6

**题目：**通过蒙特卡洛实验估计在正态分布下偏度 $\sqrt{b_1}$ 的
0.025、0.05、0.95 和 0.975
分位数。使用密度的正态近似（带有精确方差公式），根据公式 (2.14)
计算这些估计值的标准误差。将估计的分位数与大样本近似
$\sqrt{b_1} \approx N(0, 6/n)$ 的分位数进行比较。

这里我们给出公式(2.14) $$
\text{Var}(\hat{x}_q) = \frac{q(1-q)}{n f(x_q)^2},
$$ 由此公式可以计算分位数标准误差，具体代码如下：

```{r}
# 第一个函数：数据生成
generate_data <- function(m, n) {
  # 偏度函数
  sk <- function(x) {
    x_bar <- mean(x)
    b <- mean((x - x_bar)^3) / (mean((x - x_bar)^2))^1.5
    return(b)
  }

  b <- numeric(m)
  
  set.seed(0)
  for (i in 1:m) {
    X <- rnorm(n)
    b[i] <- sk(X)
  }

  # 返回排序后的b值
  b_order <- sort(b)
  
  # 内存清理（确保返回的数据不会丢失）
  rm(list = setdiff(ls(), "b_order"))
  
  return(b_order)
}

# 第二个函数：统计推断
statistical_inference <- function(b_order, n) {
  
  # 分位数计算
  qutile <- numeric(4)
  qutile[1] <- b_order[25]   # 0.025 分位数
  qutile[2] <- b_order[50]   # 0.05 分位数
  qutile[3] <- b_order[950]  # 0.95 分位数
  qutile[4] <- b_order[975]  # 0.975 分位数

  # 渐进分布密度函数
  f <- function(x, n) {
    return((1 / sqrt(2 * pi * 6 * (n - 2) / ((n + 1) * (n + 3)))) * exp(-x^2 / (2 * 6 * (n - 2) / ((n + 1) * (n + 3)))))
  }

  # 分位数标准误差计算
  sd_qutile <- numeric(4)
  sd_qutile[1] <- 0.025 * 0.975 / (n * (f(qutile[1], n))^2)
  sd_qutile[2] <- 0.05 * 0.95 / (n * (f(qutile[2], n))^2)
  sd_qutile[3] <- 0.05 * 0.95 / (n * (f(qutile[3], n))^2)
  sd_qutile[4] <- 0.025 * 0.975 / (n * (f(qutile[4], n))^2)

  # 渐近分位数计算
  cv <- numeric(4)
  cv[1] <- qnorm(0.025, 0, sqrt(6 / n))
  cv[2] <- qnorm(0.05, 0, sqrt(6 / n))
  cv[3] <- qnorm(0.95, 0, sqrt(6 / n))
  cv[4] <- qnorm(0.975, 0, sqrt(6 / n))

  # 返回所有计算结果
  inference_results <- list(qutile = qutile, sd_qutile = sd_qutile, cv = cv)
  
  # 内存清理（保留 inference_results）
  rm(list = setdiff(ls(), "inference_results"))
  
  return(inference_results)
}

# 第三个函数：结果报告
report_results <- function(inference_results) {

  # 生成结果数据框
  outcome <- data.frame(
    Estimate = c("0.025 分位数", "0.05 分位数", "0.95 分位数", "0.975 分位数"),
    MC_es = inference_results$qutile,
    asymptotic = inference_results$cv,
    MC_es_sd = inference_results$sd_qutile
  )

  # 展示结果
  print(outcome)
  
  # 内存清理
  rm(list = ls())
}

# 主程序：依次调用三个函数
main <- function() {
  m <- 1000
  n <- 1000

  # 1. 数据生成
  b_order <- generate_data(m, n)
  
  # 2. 统计推断
  inference_results <- statistical_inference(b_order, n)
  
  # 3. 结果报告
  report_results(inference_results)
}

# 执行主程序
main()

```

可以看到，估计结果与渐进表现的结果十分相近，且蒙特卡洛估计分位数的标准误差也很小。

#### 练习6.B

**题目：** 基于皮尔逊积矩相关系数 $\rho$、斯皮尔曼等级相关系数 $\rho_s$
或肯德尔系数 $\tau$ 的关联检验已在 cor.test
函数中实现。通过实证展示，当样本分布为双变量正态分布时，基于 $\rho_s$ 或
$\tau$ 的非参数检验比相关性检验的统计功效更低。找到一个备选方案（一个
$(X, Y)$ 的双变量分布，使得 $X$ 和 $Y$
是相依的），该方案下至少有一个非参数检验在与该备选方案的比较中表现出比相关性检验更高的功效。

**想法：**相较于另外两个相关系数，皮尔逊相关系数对数据条件的要求更为严格，一般来说，皮尔逊相关系数对数据条件要求如下：

(1)、两个变量之间是线性关系，都是连续数据。

(2)、两个变量的总体是正态分布，或接近正态的单峰分布。

(3)、两个变量的观测值是成对的，每对观测值之间相互独立。

因此，我思考选择一个分布，使得 $X$ 和 $Y$
之间的关系不再是简单的线性关系，而是可以通过非线性或其他复杂的关系进行关联。

这里我选择了： $X \sim N(\mu_X, \sigma_X^2)$，
$Y = e^x-3x + \epsilon$，而 $\epsilon \sim N(0, \sigma_\epsilon^2)$
是一个独立的随机误差项。

这种构造的好处在于，即使 $X$ 和 $Y$
之间存在非线性关系，斯皮尔曼和肯德尔的非参数检验仍能有效捕捉到这种依赖关系，而皮尔逊相关系数则可能低估或完全忽略这种关系。

```{r}
# 定义 x 的范围
x <- seq(-3, 3, length.out = 400)

# 计算 y = exp(x) - 3*x
y <- exp(x) - 3*x

# 绘制图像
plot(x, y, type = "l", col = "blue", lwd = 2,
     main = expression(paste("Plot of ", e^x - 3*x)),
     xlab = "x", ylab = "y")
abline(h = 0, v = 0, col = "black")  # 添加 x 和 y 轴
grid()  # 显示网格

```

```{r}
rm(list = ls()) #清除变量
set.seed(123)
# 定义功效计算函数
calculate_power <- function(cor_method = "pearson", reps = 1000, alpha = 0.05) {
  reject_null <- numeric(reps)  # 存储是否拒绝原假设的结果
  
  for (i in 1:reps) {
    # 生成非线性相关数据（作为备择假设）
    x <- rnorm(30,0,1)
    sigma <- rnorm(30,0,1)
    y <- exp(x)-3*x+sigma
    
    # 根据选择的检验方法计算相关性
    if (cor_method == "pearson") {
      test <- cor.test(x, y, method = "pearson")
    } else if (cor_method == "kendall") {
      test <- cor.test(x, y, method = "kendall")
    } else if (cor_method == "spearman") {
      test <- cor.test(x, y, method = "spearman")
    }
    
    # 如果 p 值小于显著性水平 alpha，拒绝原假设
    reject_null[i] <- test$p.value < alpha
  }
  
  # 计算功效，即拒绝原假设的频率
  power <- mean(reject_null)
  return(power)
}

# 计算不同检验方法的功效
pearson_power <- calculate_power(cor_method = "pearson")
kendall_power <- calculate_power(cor_method = "kendall")
spearman_power <- calculate_power(cor_method = "spearman")

# 打印功效
pearson_power  # Pearson 的功效
kendall_power  # Kendall 的功效
spearman_power  # Spearman 的功效

rm(list = ls()) #清除变量
```

可以看到kendall功效和Spearman 功效均大于Pearson 功效

#### ppt上作业

**题目：** 如果我们在特定的模拟设置下进行 10,000
次实验，得到两种方法的功效分别为 0.651 和
0.676。我们想知道这两种方法的功效在 0.05 的显著性水平下是否存在差异。

##### 1. **对应的假设检验问题是什么？**

在比较两种方法的功效时，我们可以提出以下假设：

**原假设（**$H_0$）：两种方法的功效是相同的，即 $p_1 = p_2$，其中 $p_1$
和 $p_2$ 分别代表两种方法的功效。

**备择假设（**$H_A$）：两种方法的功效是不同的，即
$p_1 \neq p_2$。这意味着至少有一种方法的功效显著优于另一种方法，或者两者的效果有显著差异。

##### 2. **应该使用什么检验？Z 检验、两样本 t 检验、配对 t 检验还是 McNemar 检验？为什么？**

在这个问题中，我们关注的是两种方法在相同实验条件下的功效。由于这两个样本是通过相同实验模型生成的，因此它们之间存在关联性。针对这一情况，配对
t 检验是更为合适的选择。原因如下：

-   **配对 t
    检验**：适用于比较来自同一组实验的配对数据。由于我们是在相同条件下测量两种方法的功效，因此样本间的观测值是相关的。使用配对
    t
    检验可以有效控制由于样本间差异引入的误差，使得功效的比较更具准确性。

-   **其他检验方法**：

    -   **Z
        检验**：通常用于大样本情况下的独立样本比较，但由于我们的样本是配对的，因此不适用。
    -   **两样本 t
        检验**：适合于比较两个独立样本的均值，但在这里样本是成对的。
    -   **McNemar
        检验**：通常用于比较配对数据中两个类别的变化，不适用于功效的比较。

##### 3. **假设检验所需的最少信息是什么？**

进行假设检验时，我们至少需要以下信息：

-   **样本大小（n）**：样本的重复次数，即进行蒙特卡洛模拟的次数。

-   **两个方法的功效（p1 和 p2）**：通过蒙特卡洛试验得到的功效数据，记作
    $\{p_{1i}, p_{2i}\}$ 其中 $i = 1, 2, \ldots, n$，分别代表第 $i$
    次试验中两种方法的功效值。

-   **显著性水平（**$\alpha$）：设定的显著性水平，用于判断原假设是否被拒绝。在这个例子中，显著性水平设定为
    0.05。

### Homework-5

```{r}
rm(list = ls()) #清除变量
```

#### **练习一**

在 $N = 1000$ 个假设中，950 个为原假设，50 个为备择假设。任一原假设下的
p 值服从均匀分布（使用 `runif`），任一备择假设下的 p 值服从参数为 0.1 和
1 的贝塔分布（使用 `rbeta`）。计算 Bonferroni 校正 p 值和 B-H 校正 p
值。在名义显著水平 $\alpha = 0.1$ 下，基于两种调整方法，计算 FWER、FDR
和 TPR，并基于 $m = 10000$ 次模拟重复输出这 6 个值（即 FWER、FDR 和
TPR，分别对于 Bonferroni 校正和 B-H 校正）。

```{r, eval = FALSE}
set.seed(123)
N <- 1000           # 假设总数
m <- 10000          # 模拟次数
alpha <- 0.1        # 显著性水平

# 初始化结果存储
fwer_bon <- fdr_bon <- tpr_bon <- numeric(m)
fwer_bh <- fdr_bh <- tpr_bh <- numeric(m)

for (i in 1:m) {
  # 原假设下的p值（950个）
  p_null <- runif(950)
  
  # 备择假设下的p值（50个）
  p_alt <- rbeta(50, 0.1, 1)
  
  # 所有假设的p值
  p_values <- c(p_null, p_alt)
  
  # Bonferroni校正
  bon_p <- p.adjust(p_values, method = "bonferroni")
  reject_bon <- bon_p < alpha
  fwer_bon[i] <- sum(reject_bon[1:950]) > 0  # FWER
  fdr_bon[i] <- sum(reject_bon[1:950]) / max(sum(reject_bon), 1)  # FDR
  tpr_bon[i] <- sum(reject_bon[951:1000]) / 50  # TPR
  
  # B-H校正
  bh_p <- p.adjust(p_values, method = "BH")
  reject_bh <- bh_p < alpha
  fwer_bh[i] <- sum(reject_bh[1:950]) > 0  # FWER
  fdr_bh[i] <- sum(reject_bh[1:950]) / max(sum(reject_bh), 1)  # FDR
  tpr_bh[i] <- sum(reject_bh[951:1000]) / 50  # TPR
}

# 平均结果
result <- matrix(c(mean(fwer_bon), mean(fdr_bon), mean(tpr_bon),
                   mean(fwer_bh), mean(fdr_bh), mean(tpr_bh)), 
                 nrow = 3, byrow = FALSE,
                 dimnames = list(c("FWER", "FDR", "TPR"), 
                                 c("Bonferroni", "B-H")))
print(result)

rm(list = ls()) #清除变量

```

##### 结果分析

**Bonferroni方法**是一个严格的方法，能够很好地控制假阳性（FWER），但可能会牺牲一些真正的发现（TPR），导致漏检的情况较多。

**B-H方法**则更注重在多个假设检验中发现真实的效应，能够在一定程度上控制假发现率，同时提供更高的真正率。然而，这也意味着可能有较高的FWER。

根据研究目标，选择适合的方法。如果目标是尽量找到真正的效应，而能接受一定比例的假发现，B-H方法可能是更好的选择。如果需要严格控制假阳性率，则Bonferroni方法会更为合适。

#### **题目 7.4：**

参考 **boot** 包中的空调数据集 **aircondit**。12
个观测值是空调设备之间的故障间隔时间（以小时为单位）。这些观测值为：

3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487。

假设故障间隔时间遵循指数分布模型 $\text{Exp}(\lambda)$。求解故障率
$\lambda$ 的最大似然估计（MLE），并使用 bootstrap
方法估计该估计的偏差和标准误差。

首先我们计算$\lambda$的极大似然估计：

似然函数$l(\lambda)=\sum_{i=1}^{n}p(X=x_i|\lambda)=nln(\lambda)-\lambda\sum_{i=1}^{n}x_i$

求导令导数等于0可得，该模型下$\lambda$的极大似然估计为：

$$\hat{\lambda}=\frac{1}{\bar{x}},\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i$$
在求出极大似然后，我们用bootstrap方法来给出偏差和标准误差的估计：

```{r, eval = FALSE}
# 载入所需包
library(boot)

# 故障间隔时间数据
failure_times <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)

# 对于指数分布 Exp(λ)，λ 的最大似然估计为 1/样本均值
lambda_hat <- 1 / mean(failure_times)

# 定义用于 bootstrap 的统计函数
mle_function <- function(data, indices) {
  # 使用索引 indices 进行抽样
  sample_data <- data[indices]
  # 返回 λ 的 MLE
  return(1 / mean(sample_data))
}

# 使用 bootstrap 估计偏差和标准误差
bootstrap_result <- boot(failure_times, mle_function, R = 1000)

# 输出结果
cat("MLE of λ:", lambda_hat, "\n")
cat("Bootstrap Bias:", mean(bootstrap_result$t) - lambda_hat, "\n")
cat("Bootstrap Standard Error:", sd(bootstrap_result$t), "\n")
```

#### **题目 7.5：**

参考习题 7.4。通过标准正态法、基础法、百分位法和 BCa
法计算故障间隔时间均值 $1/\lambda$ 的 95% bootstrap
置信区间。比较这些区间，并解释为什么它们可能会不同。

在这里，我们采用`boot.ci`函数，可以很简便的算出采用不同方法的置信区间。

```{r, eval = FALSE}
set.seed(123)
# 定义用于 bootstrap 的统计函数（1/λ）
mean_time_function <- function(data, indices) {
  sample_data <- data[indices]
  return(mean(sample_data))
}

# 计算不同方法的 95% 置信区间
ci_norm <- boot.ci(bootstrap_result, type = "norm")
ci_basic <- boot.ci(bootstrap_result, type = "basic")
ci_percentile <- boot.ci(bootstrap_result, type = "perc")
ci_bca <- boot.ci(bootstrap_result, type = "bca")

# 输出置信区间
cat("95% Confidence Intervals for 1/λ:\n")
cat("Normal:", ci_norm$normal[2:3], "\n")
cat("Basic:", ci_basic$basic[4:5], "\n")
cat("Percentile:", ci_percentile$percent[4:5], "\n")
cat("BCa:", ci_bca$bca[4:5], "\n")

rm(list = ls()) #清除变量
```

##### 置信区间结果分析：

在这四个置信区间中，表现最好的是BCa置信区间，具体分析如下：

**1. 正态法**

正态法依赖于正态分布的近似，即假设估计量的分布近似为正态分布。由于正态法不考虑样本分布的偏斜，如果数据存在明显的非对称性或偏斜，它可能会给出不准确的结果。而这里的分布显然不符合正态分布，所以可以看到，正太法给出的置信区间下限甚至可能是负的(而指数分布参数要求为正数)，这说明正态法表现较差。

**2. 基础法**

基础法使用了原始 bootstrap
分布中的样本对估计值进行调整，考虑到偏差。这意味着，它会将原始估计值与样本估计值的差异反向应用到
bootstrap
分布上，从而校正偏差。不过，基础法的区间可能会跨越零（如这个结果显示的负值），表明它可能不适合这个场景，尤其是当估计值不能为负（例如这里
$1/\lambda$ 是正数）。这是基础法的一个潜在缺点，尤其是对偏斜数据的表现。

**3. 百分位法**

百分位法是从 bootstrap
样本的分布中直接提取百分位数，而不进行任何偏差校正。由于它直接从数据中获得信息，因此它对偏斜的数据更敏感。它的结果与标准正态法和基础法有些不同，表明数据可能存在偏斜或异质性。百分位法对于偏斜数据通常比标准正态法更稳健，因为它没有假设正态性。

**4. BCa 法**

BCa
方法是最复杂的一种，它不仅考虑了偏差修正，还考虑了偏度修正（即加速因子，用于调整
bootstrap 分布的偏斜程度）。因此，它对偏斜数据具有更好的校正效果。BCa
法的置信区间通常被认为比其他方法更为准确，特别是在数据具有偏斜时。从结果可以看到，BCa
法的区间比其他三种方法更加集中，且不像正态、基础法那样给出了负值。

#### **练习4**

假设总体服从参数为λ的指数分布，则λ的最大似然估计MLE为$\hat{\lambda}$ =
$\frac{1}{\bar{X}}$，其中$\bar{X}$是样本均值。可以推导出$\hat{\lambda}$的期望为$\lambda n/(n - 1)$，因此估计偏差为$\lambda/(n - 1)$。$\hat{\lambda}$的标准误为$λn/[(n - 1)\sqrt{(n - 2)}]$。通过模拟研究验证Bootstrap方法的性能，对比Bootstrap方法和理论结果差距。

(这个题目我看作业里面没写，但ppt上有，所以还是做了)

```{r, eval = FALSE}
set.seed(123)

# 参数设定
lambda_true <- 2    # 真正的lambda值
n_values <- c(5, 10, 20)  # 样本量
B <- 1000  # Bootstrap重复次数
m <- 1000  # 模拟次数

# 存储结果的矩阵
results <- data.frame(n = integer(0), 
                      mean_bootstrap = numeric(0),
                      mean_bias_bootstrap = numeric(0), 
                      mean_bias_theoretical = numeric(0), 
                      se_bootstrap = numeric(0), 
                      se_theoretical = numeric(0))

# 循环遍历不同的样本量n
for (n in n_values) {
  
  # 初始化存储偏差和标准误的向量
  bootstrap <- numeric(m)
  bias_bootstrap <- numeric(m)
  se_bootstrap <- numeric(m)
  
  # 模拟过程
  for (i in 1:m) {
    # 生成样本数据，来自指数分布，参数lambda_true
    sample_data <- rexp(n, rate = lambda_true)
    
    # 计算MLE的lambda估计值
    lambda_hat <- 1 / mean(sample_data)
    
    # 进行B次Bootstrap抽样
    bootstrap_estimates <- numeric(B)
    for (b in 1:B) {
      bootstrap_sample <- sample(sample_data, size = n, replace = TRUE)
      bootstrap_estimates[b] <- 1 / mean(bootstrap_sample)
    }
    
    # 计算Bootstrap偏差和标准误
    bootstrap[i] <- mean(bootstrap_estimates)
    bias_bootstrap[i] <- mean(bootstrap_estimates) - lambda_hat
    se_bootstrap[i] <- sd(bootstrap_estimates)
  }
  
  # 理论偏差和标准误
  theoretical_bias <- lambda_true / (n - 1)
  theoretical_se <- lambda_true * n / ((n - 1) * sqrt(n - 2))
  
  # 计算平均Bootstrap偏差和标准误
  mean_bootstrap <- mean(bootstrap)
  mean_bias_bootstrap <- mean(bias_bootstrap)
  mean_se_bootstrap <- mean(se_bootstrap)
  
  # 将结果存储到数据框中
  results <- rbind(results, data.frame(
    n = n,
    mean_bootstrap = mean_bootstrap,
    mean_bias_bootstrap = mean_bias_bootstrap,
    mean_bias_theoretical = theoretical_bias,
    se_bootstrap = mean_se_bootstrap,
    se_theoretical = theoretical_se
  ))
}

# 打印结果
print(results)

rm(list = ls()) #清除变量
```

从结果可以看出，随着样本量$n$的增加，Bootstrap方法的表现逐渐改善。这说明Bootstrap方法本身并不能完全解决小样本量带来的问题。因此，在使用Bootstrap方法时，仍然需要确保原始样本量足够大，否则样本过少可能导致估计结果偏离总体特征。即使通过Bootstrap进行重抽样，也难以获得准确的估计。

### Homework-6

```{r}
rm(list = ls()) #清除变量
```

#### 练习9.3

**题目：** 使用 Metropolis-Hastings
采样器从标准柯西分布生成随机变量。丢弃链中前 1000
个值，并将生成的观测值的分位数与标准柯西分布的分位数进行比较（参见
`qcauchy` 或 `qt`，自由度 `df=1`）。回忆一下，柯西分布
$\text{Cauchy}(\theta, \eta)$ 的概率密度函数为： $$
f(x) = \frac{1}{\theta \pi (1 + \left(\frac{x - \eta}{\theta}\right)^2)}, \quad -\infty < x < \infty, \quad \theta > 0。
$$ 标准柯西分布是指参数为 $\text{Cauchy}(\theta = 1, \eta = 0)$ 的分布。

```{r}
set.seed(123)

library(ggplot2)

Metropolis <- function(m, f, g_sd, sigm) {
  k <- 0
  x <- numeric(m)
  x[1] <- rnorm(1, mean = 0, sd = sigm)  # 初始化采样起点
  u <- runif(m)  # 接受-拒绝随机数

  for (i in 2:m) {
    xt <- x[i - 1]
    y <- rnorm(1, mean = xt, sd = sigm)  # 候选样本
    num <- f(y) * dnorm(xt, mean = y, sd = sigm)
    den <- f(xt) * dnorm(y, mean = xt, sd = sigm)
    
    if (u[i] <= num / den) {
      x[i] <- y
    } else {
      x[i] <- xt
      k <- k + 1
    }
  }
  cat("Reject probability:", k / m, "\n")
  return(x)
}

# 参数设置
m <- 100000
burn <- 1000
sigm <- c(0.05, 0.5, 1, 2, 16)
f <- function(x) { dcauchy(x) }  # 标准柯西分布
g_sd <- 1  # 正态分布标准差

# 运行 Metropolis-Hastings 采样
x <- Metropolis(m, f, g_sd, sigm[4])

DrawQQPlot <- function(x, burn, scale) {
  # 去除烧入期的样本
  x <- sort(x[(burn + 1):length(x)])
  
  # 生成与样本数相同的标准柯西分布样本，并排序
  y <- sort(rcauchy(length(x)))
  
  # 计算分位数
  quantileX <- quantile(x, probs = seq(0, 1, 0.01))
  quantileY <- quantile(y, probs = seq(0, 1, 0.01))
  
  # 绘制 Q-Q 图
  plot(quantileY, quantileX, col = "blue", pch = 16,
       xlim = scale, ylim = scale,
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
  
  # 添加参考线
  abline(0, 1, col = "red", lwd = 2)
}

# 设置参数并调用函数
burn <- 1000
scale <- c(-5, 5)  # 可根据需求调整
DrawQQPlot(x, burn, scale)

# 去除烧入期样本
samples <- x[(burn + 1):m]

# 计算生成样本和标准柯西分布的分位数
sample_deciles <- quantile(samples, probs = seq(0.1, 0.9, by = 0.1))
cauchy_deciles <- qcauchy(seq(0.1, 0.9, by = 0.1))

# 比较生成样本的分位数和理论分位数
comparison <- data.frame(
  Decile = seq(0.1, 0.9, by = 0.1),
  Sample_Deciles = sample_deciles,
  Cauchy_Deciles = cauchy_deciles
)

print(comparison)

# 绘制直方图与密度图
ggplot(data.frame(samples = samples), aes(x = samples)) +
  geom_histogram(aes(y = ..density..), bins = 100, color = "black", fill = "skyblue") +
  geom_density(color = "red") +
  ggtitle("Histogram of Samples from Cauchy Distribution (after burn-in)") +
  xlab("Value") +
  ylab("Density")
 
```

从十分位数对比结果和QQ图可以看出来，MH算法在靠近中心的
(0,0)附近，拟合状况较好，分布的拟合状况像两侧逐渐偏移。整体来看，分布拟合状况良好，这从我们画的直方图也可以看出来。

```{r}
rm(list = ls()) #清除变量
```

#### 练习9.8

**题目：** 考虑一个二元密度函数：

$$
f(x, y) \propto \binom{n}{x} y^{x + a - 1} (1 - y)^{n - x + b - 1}, \quad x = 0, 1, \dots, n, \quad 0 \leq y \leq 1.
$$

可以证明，对于固定的 $a$、$b$、$n$，其条件分布分别为二项分布
$\text{Binomial}(n, y)$ 和贝塔分布
$\text{Beta}(x + a, n - x + b)$。使用吉布斯采样器生成以目标联合密度
$f(x, y)$ 为目标的链。

```{r}
# 设置参数
a <- 2     # 参数 a
b <- 2     # 参数 b
n <- 10    # 二项分布的样本数

# 设置 Gibbs 采样的初始值和迭代次数
num_iterations <- 5000
samples_x <- numeric(num_iterations)
samples_y <- numeric(num_iterations)
samples_x[1] <- 5   # 初始 x 值
samples_y[1] <- 0.5 # 初始 y 值

# Gibbs 采样过程
for (i in 2:num_iterations) {
  # 给定 y 条件下，从二项分布采样 x
  samples_x[i] <- rbinom(1, n, samples_y[i - 1])
  
  # 给定 x 条件下，从 Beta 分布采样 y
  samples_y[i] <- rbeta(1, samples_x[i] + a, n - samples_x[i] + b)
}

# 绘制样本结果
par(mfrow = c(1, 2))
plot(samples_x, type = "l", col = "blue", main = "样本 x 的轨迹", xlab = "迭代次数", ylab = "x 值")
plot(samples_y, type = "l", col = "red", main = "样本 y 的轨迹", xlab = "迭代次数", ylab = "y 值")

# 计算和展示联合分布的样本
plot(samples_x, samples_y, pch = 16, col = rgb(0, 0, 1, 0.5), 
     main = "联合分布样本 (x, y)", xlab = "x", ylab = "y")
```

```{r}
rm(list = ls()) #清除变量
```

#### 练习三

**题目：** 利用Gelman-Rubin检验练习一和练习二得到的马氏链是否收敛。

这里我们分为3.1和3.2，3.1是练习一对应的Gelman-Rubin检验，3.2是练习二对应的Gelman-Rubin检验。

##### 3.1

```{r}
# 定义目标分布的密度函数（标准柯西分布）
target_density <- function(x) {
  1 / (pi * (1 + x^2))
}

# Metropolis-Hastings 采样函数
metropolis_hastings <- function(iter, proposal_sd = 1, init_val = 0) {
  samples <- numeric(iter)
  samples[1] <- init_val
  for (i in 2:iter) {
    proposal <- rnorm(1, mean = samples[i - 1], sd = proposal_sd)
    accept_prob <- min(1, target_density(proposal) / target_density(samples[i - 1]))
    if (runif(1) < accept_prob) {
      samples[i] <- proposal
    } else {
      samples[i] <- samples[i - 1]
    }
  }
  return(samples)
}

# Gelman-Rubin 检验函数
Gelman.Rubin <- function(psi) {
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)

  psi.means <- rowMeans(psi)     # 每条链的均值
  B <- n * var(psi.means)        # 链间方差
  psi.w <- apply(psi, 1, "var")  # 链内方差
  W <- mean(psi.w)               # 链内方差的平均值
  v.hat <- W * (n - 1) / n + B / n  # 联合后验分布的估计方差
  r.hat <- v.hat / W             # Gelman-Rubin 统计量
  return(r.hat)
}

# 设置参数
n_samples <- 31000
burn_in <- 1000
num_chains <- 4
proposal_sd <- 1
initial_values <- c(-10, -5, 5, 10)  # 不同的初始值

# 生成多个链
set.seed(12345)
chains <- matrix(0, nrow = num_chains, ncol = n_samples - burn_in)
for (i in 1:num_chains) {
  chain <- metropolis_hastings(n_samples, proposal_sd, init_val = initial_values[i])
  chains[i, ] <- chain[(burn_in + 1):n_samples]
}

# 计算每条链的累积均值
psi <- t(apply(chains, 1, cumsum))
for (i in 1:nrow(psi)) {
  psi[i,] <- psi[i,] / (1:ncol(psi))
}

# 计算 Gelman-Rubin 统计量随采样点数量的变化
R_values <- numeric(n_samples - burn_in )
for (j in 1:(n_samples -  burn_in)) {
  R_values[j] <- Gelman.Rubin(psi[, 1:j])
}

# 绘制 Gelman-Rubin 统计量随采样点数量变化的图像
plot(R_values, type = 'l', col = 'purple', xlab = 'Number of Sampling Points (n)',
     ylab = 'R Statistic', main = 'Gelman-Rubin Diagnostic R vs. Number of Sampling Points')
abline(h = 1.2, col = 'red', lty = 2)  # 添加 R = 1.2 的参考线
legend("topright", legend = c("R Statistic", "R = 1.2"), col = c("purple", "red"), lty = c(1, 2))

```

```{r}

# 绘制每条链的累积均值随采样点数量变化的图像
plot(1:(n_samples - burn_in), psi[1, ], type = "l", col = "blue", ylim = range(psi),
     xlab = "Number of Sampling Points (n)", ylab = "Cumulative Mean",
     main = "Cumulative Mean of Each Chain")
for (i in 2:num_chains) {
  lines(1:(n_samples - burn_in), psi[i, ], col = i)
}
legend("topright", legend = paste("Chain", 1:num_chains), col = 1:num_chains, lty = 1)

```

```{r}
rm(list = ls()) #清除变量
```

观察结果我们可以看到，在生成样本数量为6000左右时，利用Gelman-Rubin检验方法得到的R小于1.2，但随着样本数量n的增大，不同链又呈现发散状态，得到的R大于1.2，而这与每条链的均值图表现也十分吻合，推测发生这种情况原因是在用MH算法生成cauchy分布样本时，选择的函数会影响结果(例如这里我们选择的就是标准正态分布)。

##### 3.2

```{r, eval = FALSE}
# 设置 Gibbs 采样器函数
gibbs_sampler <- function(num_iterations, n, a, b, init_x, init_y) {
  samples_x <- numeric(num_iterations)
  samples_y <- numeric(num_iterations)
  samples_x[1] <- init_x
  samples_y[1] <- init_y
  
  for (i in 2:num_iterations) {
    # 给定 y 条件下，从二项分布采样 x
    samples_x[i] <- rbinom(1, n, samples_y[i - 1])
    
    # 给定 x 条件下，从 Beta 分布采样 y
    samples_y[i] <- rbeta(1, samples_x[i] + a, n - samples_x[i] + b)
  }
  
  return(list(samples_x = samples_x, samples_y = samples_y))
}

# Gelman-Rubin 检验函数
gelman_rubin <- function(chains) {
  m <- ncol(chains)  # 链的数量
  n <- nrow(chains)  # 每条链的采样点数量
  
  # Step 1: 计算每条链的均值
  chain_means <- colMeans(chains)
  
  # Step 2: 计算链间方差 B
  overall_mean <- mean(chain_means)
  B <- n * sum((chain_means - overall_mean)^2) / (m - 1)
  
  # Step 3: 计算每条链内的方差 W
  W <- sum(apply(chains, 2, var)) / m
  
  # Step 4: 估计联合后验分布的方差 V_hat
  V_hat <- (1 - 1/n) * W + (1/n) * B
  
  # Step 5: 计算 R 统计量
  R_hat <- sqrt(V_hat / W)
  
  return(R_hat)
}

# 设置参数
a <- 2
b <- 2
n <- 10
num_iterations <- 10000

# 生成 3 条链
set.seed(123)
chain1 <- gibbs_sampler(num_iterations, n, a, b, init_x = 5, init_y = 0.5)
chain2 <- gibbs_sampler(num_iterations, n, a, b, init_x = 6, init_y = 0.6)
chain3 <- gibbs_sampler(num_iterations, n, a, b, init_x = 4, init_y = 0.4)

# 仅对 y 值进行 Gelman-Rubin 检验
chains_y <- cbind(chain1$samples_y, chain2$samples_y, chain3$samples_y)

# 检验结果
R_hat <- gelman_rubin(chains_y)
cat("Gelman-Rubin 诊断的 R 统计量:", R_hat, "\n")

# 检查 R 统计量是否小于 1.2
if (R_hat < 1.2) {
  cat("链已收敛。\n")
} else {
  cat("链未收敛。\n")
}
```

```{r, eval = FALSE}
# 设置 Gibbs 采样器函数
gibbs_sampler <- function(num_iterations, n, a, b, init_x, init_y) {
  samples_x <- numeric(num_iterations)
  samples_y <- numeric(num_iterations)
  samples_x[1] <- init_x
  samples_y[1] <- init_y
  
  for (i in 2:num_iterations) {
    # 给定 y 条件下，从二项分布采样 x
    samples_x[i] <- rbinom(1, n, samples_y[i - 1])
    
    # 给定 x 条件下，从 Beta 分布采样 y
    samples_y[i] <- rbeta(1, samples_x[i] + a, n - samples_x[i] + b)
  }
  
  return(samples_y)
}

# 设置参数
num_iterations <- 10000
n <- 10
a <- 2
b <- 2

# 生成 3 条链
set.seed(123)  # 设置随机种子以保持可重复性
chain1_y <- gibbs_sampler(num_iterations, n, a, b, init_x = 5, init_y = 0.5)
chain2_y <- gibbs_sampler(num_iterations, n, a, b, init_x = 6, init_y = 0.6)
chain3_y <- gibbs_sampler(num_iterations, n, a, b, init_x = 4, init_y = 0.4)

# 计算每条链的均值
chain_means1 <- cumsum(chain1_y) / (1:num_iterations)
chain_means2 <- cumsum(chain2_y) / (1:num_iterations)
chain_means3 <- cumsum(chain3_y) / (1:num_iterations)

# 绘图
plot(chain_means1, type = 'l', col = 'blue', ylim = range(c(chain_means1, chain_means2, chain_means3)),
     xlab = 'Number of Sampling Points (n)', ylab = 'Mean of Samples', main = 'Chain Means vs. Number of Sampling Points')
lines(chain_means2, col = 'orange')
lines(chain_means3, col = 'green')

# 添加最终均值的虚线
abline(h = mean(chain1_y), col = 'blue', lty = 2)
abline(h = mean(chain2_y), col = 'orange', lty = 2)
abline(h = mean(chain3_y), col = 'green', lty = 2)

# 添加图例
legend("topright", legend = c("Chain 1 Mean", "Chain 2 Mean", "Chain 3 Mean", "Chain 1 Final Mean", 
                               "Chain 2 Final Mean", "Chain 3 Final Mean"),
       col = c("blue", "orange", "green", "blue", "orange", "green"), 
       lty = c(1, 1, 1, 2, 2, 2))
```

```{r, eval = FALSE}
# Gelman-Rubin 检验函数
Gelman.Rubin <- function(psi) {
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)

  psi.means <- rowMeans(psi)     # 每条链的均值
  B <- n * var(psi.means)        # 链间方差
  psi.w <- apply(psi, 1, "var")  # 每条链内的方差
  W <- mean(psi.w)               # 链内方差的平均值
  v.hat <- W * (n - 1) / n + B / n  # 联合后验分布的估计方差
  r.hat <- v.hat / W             # Gelman-Rubin 统计量
  return(r.hat)
}

# Gibbs 采样函数
gibbs_sampler <- function(num_iterations, n, a, b, init_x, init_y) {
  samples_x <- numeric(num_iterations)
  samples_y <- numeric(num_iterations)
  samples_x[1] <- init_x
  samples_y[1] <- init_y
  
  for (i in 2:num_iterations) {
    # 给定 y 条件下，从二项分布采样 x
    samples_x[i] <- rbinom(1, n, samples_y[i - 1])
    
    # 给定 x 条件下，从 Beta 分布采样 y
    samples_y[i] <- rbeta(1, samples_x[i] + a, n - samples_x[i] + b)
  }
  
  return(samples_y)
}

# 设置参数
a <- 2
b <- 2
n <- 10
num_iterations <- 5000
k <- 4  # 生成的链数
burn_in <- 1000  # 烧入期

# 设置不同链的初始值
initial_values <- list(c(5, 0.5), c(6, 0.6), c(4, 0.4), c(7, 0.3))

# 生成 k 条链
set.seed(12345)
chains <- matrix(0, nrow = k, ncol = num_iterations)
for (i in 1:k) {
  init_x <- initial_values[[i]][1]
  init_y <- initial_values[[i]][2]
  chains[i, ] <- gibbs_sampler(num_iterations, n, a, b, init_x, init_y)
}

# 计算每条链的累积均值
psi <- t(apply(chains, 1, cumsum))
for (i in 1:nrow(psi)) {
  psi[i,] <- psi[i,] / (1:ncol(psi))
}

# Gelman-Rubin 检验
R_values <- numeric(num_iterations)
for (j in (burn_in + 1):num_iterations) {
  R_values[j] <- Gelman.Rubin(psi[, 1:j])
}

# 绘制 R 统计量随采样点数量变化的图像
plot(R_values[(burn_in + 1):num_iterations], type = 'l', col = 'purple',
     xlab = 'Number of Sampling Points (n)', ylab = 'R Statistic',
     main = 'Gelman-Rubin Diagnostic R vs. Number of Sampling Points')
abline(h = 1.2, col = 'red', lty = 2)  # 添加 R = 1.2 的参考线
legend("topright", legend = c("R Statistic", "R = 1.2"), col = c("purple", "red"), lty = c(1, 2))

```

在利用Gelman-Rubin检验判断练习二的马氏链时，可以看到，当n大于2000时(1000+burn-in)，得到的R就已经小于1.2了，同时观察不同链的均值情况，我们发现练习二得到的马氏链收俩较快且效果较好，这体现了Gibbs抽样的优越性。

#### 练习4

证明MH算法的细致平稳条件

##### Metropolis-Hastings采样算法的流程

1.  初始化时间 $t = 1$\
2.  设置 $u$ 的值，并初始化初始状态 $\theta^{(t)} = u$\
3.  重复以下的过程：
    -   令 $t = t + 1$\
    -   从已知分布 $q\left(\theta \mid \theta^{(t-1)}\right)$
        中生成一个候选状态 $\theta^{(*)}$\
    -   计算接受的概率：\
        $$
        \alpha = \min\left(1, \frac{p\left(\theta^{(*)}\right) q\left(\theta^{(t-1)} \mid \theta^{(*)}\right)}{p\left(\theta^{(t-1)}\right) q\left(\theta^{(*)} \mid \theta^{(t-1)}\right)}\right)
        $$
    -   从均匀分布 $U(0, 1)$ 生成一个随机值 $a$\
    -   如果
        $a < \alpha$，接受更新生成的值：$\theta^{(t)} = \theta^{(*)}$；否则：$\theta^{(t)} = \theta^{(t-1)}$\
4.  直到 $t = T$

##### Metropolis-Hastings采样算法平稳性证明

要证明Metropolis-Hastings采样算法的收敛性，最重要的是要证明构造的马尔可夫过程满足细致平稳条件，即：
$$
\pi(i) P_{i,j} = \pi(j) P_{j,i}
$$

对于上面所述的过程，分布为 $p(\theta)$，从状态 $i$ 转移到状态 $j$
的转移概率为： $$
P_{i,j} = \alpha_{i,j} \cdot Q_{i,j}
$$ 其中，$Q_{i,j}$ 为上述已知的分布，$Q_{i,j}$ 为： $$
Q_{i,j} = q\left(\theta^{(j)} \mid \theta^{(i)}\right)
$$

接下来，我们证明在Metropolis-Hastings采样算法中构造的马尔可夫链满足细致平稳条件。
$$
p\left(\theta^{(i)}\right) P_{i,j} = p\left(\theta^{(i)}\right) \cdot \alpha_{i,j} \cdot Q_{i,j}
$$ $$
= p\left(\theta^{(i)}\right) \cdot \min\left(1, \frac{p\left(\theta^{(j)}\right) \cdot q\left(\theta^{(i)} \mid \theta^{(j)}\right)}{p\left(\theta^{(i)}\right) \cdot q\left(\theta^{(j)} \mid \theta^{(i)}\right)}\right) \cdot q\left(\theta^{(j)} \mid \theta^{(i)}\right)
$$ $$
= \min\left(p\left(\theta^{(i)}\right) \cdot q\left(\theta^{(j)} \mid \theta^{(i)}\right), p\left(\theta^{(j)}\right) \cdot q\left(\theta^{(i)} \mid \theta^{(j)}\right)\right)
$$ $$
= p\left(\theta^{(j)}\right) \cdot \min\left(1, \frac{p\left(\theta^{(i)}\right) \cdot q\left(\theta^{(j)} \mid \theta^{(i)}\right)}{p\left(\theta^{(j)}\right) \cdot q\left(\theta^{(i)} \mid \theta^{(j)}\right)}\right) \cdot q\left(\theta^{(i)} \mid \theta^{(j)}\right)
$$ $$
= p\left(\theta^{(j)}\right) \cdot \alpha_{j,i} \cdot Q_{j,i}
$$ $$
= p\left(\theta^{(j)}\right) P_{j,i}
$$

因此，通过以上的方法构造出来的马尔可夫链是满足细致平稳条件的。

### Homework-7

```{r}
rm(list = ls()) #清除变量
```

#### 练习一

**题目:**设计算法以计算以下和的第 $k$ 项：

$$
\sum_{k=0}^{\infty} \frac{(-1)^k \|a\|^{2k+2}}{k! \cdot 2^k \cdot (2k+1)(2k+2)} \frac{\Gamma\left(\frac{d+1}{2}\right) \Gamma\left(k + \frac{3}{2}\right)}{\Gamma\left(k + \frac{d}{2} + 1\right)}
$$

其中 $d \geq 1$ 是一个整数，$a$ 是 $\mathbb{R}^d$
中的一个向量，$\| \cdot \|$ 表示欧几里得范数。

1.  **(a)** 编写一个函数以计算第 $k$ 项。
2.  **(b)** 修改函数，使其计算并返回整个和。
3.  **(c)** 当 $a = (1, 2)^T$ 时，计算该和的值。

```{r}
library(pracma)

# 定义计算第 k 项的函数
compute_kth_term <- function(k, d, a) {
  # 计算范数 ||a||
  norm_a <- norm(a, type = "2")
  
  # 计算第 k 项的分子和分母
  numerator <- (-1)^k * norm_a^(2*k + 2) * gamma((d + 1) / 2) * gamma(k + 3 / 2)
  denominator <- factorial(k) * 2^k * (2 * k + 1) * (2 * k + 2) * gamma(k + d / 2 + 1)
  
  # 返回第 k 项的值
  return(numerator / denominator)
}

# 计算整个和的函数
compute_sum <- function(d, a, tol = 1e-10) {
  sum_value <- 0
  k <- 0
  
  # 迭代求和，直到第 k 项小于容差 tol
  repeat {
    term <- compute_kth_term(k, d, a)
    sum_value <- sum_value + term
    if (abs(term) < tol) break
    k <- k + 1
  }
  
  return(sum_value)
}

# 参数设置
d <- 2
a <- c(1, 2)

# 计算和的值
result <- compute_sum(d, a)
cat("和的值为:", result, "\n")
```

本题中我们采用`compute_kth_term` 函数计算给定 $k$ 值时的第 $k$ 项，
`compute_sum`
函数迭代地计算每一项的和，直到当前项小于给定的容差（默认值为
$10^{-10}$）为止。

```{r}
rm(list = ls()) #清除变量
```

#### 练习二

**题目：** 写一个函数来求解如下关于 $a$ 的方程：

$$
\frac{2\Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi (k-1)}\Gamma\left(\frac{k-1}{2}\right)} \int_0^{c_k - 1} \left(1 + \frac{u^2}{k-1}\right)^{-\frac{k}{2}} \,du = \frac{2\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k}\Gamma\left(\frac{k}{2}\right)} \int_0^{c_k} \left(1 + \frac{u^2}{k}\right)^{-\frac{k+1}{2}} \,du,
$$

其中：

$$
c_k = \sqrt{\frac{a^2 k}{k + 1 - a^2}}.
$$

要求对比求解出的 $a$ 值与习题 11.4 中的 $A(k)$ 进行比较。

首先我们给出**11.4**的解答：

```{r}
# 加载必要的库
#if(!requireNamespace("stats", quietly = TRUE)) install.packages("stats")
library(stats)

# 定义概率函数
Sk_minus_1 <- function(a, k) {
  threshold <- sqrt(a^2 * (k - 1) / (k - a^2))
  1 - pt(threshold, df = k - 1)
}

Sk <- function(a, k) {
  threshold <- sqrt(a^2 * k / (k + 1 - a^2))
  1 - pt(threshold, df = k)
}

# 定义找到交点的函数
find_intersection <- function(k, tol = 1e-6) {
  # 定义目标函数，求解使 Sk_minus_1 和 Sk 接近的 a 值
  target_function <- function(a) abs(Sk_minus_1(a, k) - Sk(a, k))
  
  # 使用 optimize 函数在 (0, sqrt(k)) 区间内找到最小值
  result <- optimize(target_function, interval = c(0, sqrt(k)), tol = tol)
  a_opt <- result$minimum
  return(a_opt)
}

# 计算并打印 k = 100, 150, 200 对应的交点 A(k)
k_values <- c(100,150,200)
A_k <- sapply(k_values, find_intersection)

# 显示结果
for (i in 1:length(k_values)) {
  cat("k =", k_values[i], ", A(k) =", A_k[i], "\n")
}
```

```{r}
rm(list = ls()) #清除变量
```

**11.5**解答，使用 `integrate()` 函数进行数值积分，`uniroot()`
函数求解方程：

**一个小问题：**在使用`uniroot()`
函数求解方程时，在本题，我遇到了如下报错："位于极点边的f()值之正负号不相反",然而我通过观察`target_function`图像时，发现它其实是存在两边正负值相反的极值点的，但由于其与0的距离相差很近(比如可能是1e-12)，超出了`uniroot()`的判断范围，对于这个问题，我采取的做法是放大`target_function`,具体的放大倍数可由下面的绘制图像来确定。

```{r}
# 定义目标函数
k=100
ck <- function(a, k) {
    sqrt(a^2 * k / (k + 1 - a^2))
  }
left_integral <- function(a, k) {
    ck_val <- ck(a, k)
      result <- integrate(function(u) (1 + u^2 / (k - 1))^(-k / 2), 0, ck_val - 1)$value
      coef <- 2 * gamma(k / 2) / (sqrt(pi * (k - 1)) * gamma((k - 1) / 2))
      return(coef * result)
    }
  
right_integral <- function(a, k) {
    ck_val <- ck(a, k)
      result <- integrate(function(u) (1 + u^2 / k)^(-(k + 1) / 2), 0, ck_val)$value
      coef <- 2 * gamma((k + 1) / 2) / (sqrt(pi * k) * gamma(k / 2))
      return(coef * result)
    }
target_function <- function(a) {
  (left_integral(a, k) - right_integral(a, k))*1e14
}

# 绘制目标函数的值在区间的变化情况
a_values <- seq(6.7,6.9, length.out = 1000)
target_values <- sapply(a_values, target_function)

plot(a_values, target_values, type = "l", col = "blue", lwd = 2,
     xlab = "a", ylab = "target_function(a)", main = "Plot of target_function(a)")
abline(h = 0, col = "red", lty = 2)  # 添加 y = 0 的水平线

```

这里以'k=100'为例，此时我们通过观察图像，发现放大1e14是较好的，同时确定了k值是属于(6.7,6.9),于是在下面的`uniroot()`
函数中设置适当的区间和`target_function`

```{r}
# 加载所需的包
library(stats)

# 定义方程函数
solve_equation <- function(k) {
  # 定义 c_k 的函数
  ck <- function(a, k) {
    sqrt(a^2 * k / (k + 1 - a^2))
  }
  
  # 定义积分函数
  left_integral <- function(a, k) {
    ck_val <- ck(a, k)
      result <- integrate(function(u) (1 + u^2 / (k - 1))^(-k / 2), 0, ck_val - 1)$value
      coef <- 2 * gamma(k / 2) / (sqrt(pi * (k - 1)) * gamma((k - 1) / 2))
      return(coef * result)
    }
  
  right_integral <- function(a, k) {
    ck_val <- ck(a, k)
      result <- integrate(function(u) (1 + u^2 / k)^(-(k + 1) / 2), 0, ck_val)$value
      coef <- 2 * gamma((k + 1) / 2) / (sqrt(pi * k) * gamma(k / 2))
      return(coef * result)
    }
  
  # 定义求解的目标函数
  target_function <- function(a) {
    (left_integral(a, k) - right_integral(a, k))*1e14
  }
  
  # 使用 uniroot() 求解方程，扩大求解区间
  solution <- uniroot(target_function, interval = c(6.7,6.9))
  return(solution$root)
}

k <- 100
a_value <- solve_equation(k)
cat("当 k =", k, "时，求解出的 a 值为：", a_value, "\n")


```

采用适当的区间和放大倍数，我们可以得到，当 k = 100 时，求解出的 a 值为：
6.796032 ，当 k = 100 时，求解出的 a 值为： 7.522129 ,当 k = 200
时，求解出的 a 值为： 8.719138 。
对比11.4的结果我们发现，k值越大，11.5的结果和11.4的结果越接近。

```{r}
rm(list = ls()) #清除变量
```

#### 练习三

假设 $T_1, \ldots, T_n$ 是来自均值为 $\lambda$
的指数分布的独立同分布样本。由于右截尾，大于 $\tau$
的值未被观察到，因此观测值为： $$
Y_i = T_i I(T_i \leq \tau) + \tau I(T_i > \tau), \quad i = 1, \ldots, n.
$$ 设 $\tau = 1$，并给定观测值 $Y_i$ 如下：

$$
0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85
$$

使用E-M算法估计
$\lambda$，并将结果与观测数据的最大似然估计进行比较（注意：$Y_i$
服从混合分布）。

```{r}
# 观测数据
Y <- c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
tau <- 1  # 截尾点

# 初始化
n <- length(Y)
lambda_est <- 1 / mean(Y)  # 初始估计值

# E-M算法
epsilon <- 1e-6  # 收敛条件
max_iter <- 1000  # 最大迭代次数
for (i in 1:max_iter) {
  # E步：计算未观测部分的期望
  expected_T <- ifelse(Y < tau, Y, tau + 1 / lambda_est)
  
  # M步：更新 lambda 的估计值
  new_lambda_est <- n / sum(expected_T)
  
  # 检查收敛条件
  if (abs(new_lambda_est - lambda_est) < epsilon) {
    break
  }
  lambda_est <- new_lambda_est
}

# 输出E-M算法估计结果
cat("E-M算法估计的lambda:", lambda_est, "\n")

# 直接的观测数据MLE估计
lambda_mle <- 1 / mean(Y)
cat("观测数据的MLE估计lambda:", lambda_mle, "\n")
rm(list = ls()) #清除变量
```

可以看到，E-M算法估计的lambda为1.037038，与真实值1较为接近，而直接采用MLE得到的结果是
1.481481 ，与真实值相差较远。

### Homework-8

```{r}
rm(list = ls()) #清除变量
```

#### 练习一

使用单纯形算法求解以下问题,最小化 $4x + 2y + 9z$，满足

$$
2x + y + z \leq 2  \\
x - y + 3z \leq 3  \\
x \geq 0, \quad y \geq 0, \quad z \geq 0.
$$

```{r}
# 加载boot包
library(boot)

# 目标函数系数
a <- c(4, 2, 9)

# 约束矩阵（注意负号，因为simplex默认使用 <= 形式的约束）
A1 <- rbind(c(2, 1, 1), c(1, -1, 3))

# 约束条件的右侧值
b1 <- c(2, 3)

# 求解最小化问题
result <- simplex(a = a, A1 = A1, b1 = b1, maxi = FALSE)

# 输出结果
result

```

可以看到最小值是0，在x=y=z=0时取到。

```{r}
rm(list = ls()) #清除变量
```

#### 练习二

**习题 3**\
使用 `for` 循环和 `lapply()` 来拟合线性模型，数据集为 `mtcars`

**习题 4**\
对 `mtcars` 数据集的每个 bootstrap 样本，使用 `for` 循环和 `lapply()`
拟合模型 $\text{mpg} \sim \text{disp}$。

**习题 5**\
对于前两题中的每个模型，使用给定函数提取 $R^2$ 值：

```{r}
# 习题 3
# 定义公式列表
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

# 使用for循环拟合线性模型
models_for <- list()
for (i in seq_along(formulas)) {
  models_for[[i]] <- lm(formulas[[i]], data = mtcars)
}

# 使用lapply拟合线性模型
models_lapply <- lapply(formulas, function(f) lm(f, data = mtcars))

# 习题 4
# 生成bootstrap样本
bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), replace = TRUE)
  mtcars[rows, ]
})

# 使用for循环拟合模型 mpg ~ disp
models_boot_for <- list()
for (i in seq_along(bootstraps)) {
  models_boot_for[[i]] <- lm(mpg ~ disp, data = bootstraps[[i]])
}

# 使用lapply拟合模型 mpg ~ disp
models_boot_lapply <- lapply(bootstraps, function(data) lm(mpg ~ disp, data = data))

# 习题 5
# 定义提取 R^2 的函数
rsq <- function(mod) summary(mod)$r.squared

# 提取习题 3 中的模型的 R^2
rsq_for <- sapply(models_for, rsq)
rsq_lapply <- sapply(models_lapply, rsq)

# 提取习题 4 中的 bootstrap 模型的 R^2
rsq_boot_for <- sapply(models_boot_for, rsq)
rsq_boot_lapply <- sapply(models_boot_lapply, rsq)

# 输出结果
list(
  rsq_for = rsq_for,
  rsq_lapply = rsq_lapply,
  rsq_boot_for = rsq_boot_for,
  rsq_boot_lapply = rsq_boot_lapply
)
```

可以看到，采用for循环和`lapply()`函数得到的模型一样，所求出的$R^2$相同。

```{r}
rm(list = ls()) #清除变量
```

#### 练习三

**习题 3**\
以下代码模拟了在非正态数据上的t检验表现。使用 `sapply()`
和匿名函数从每个试验中提取 p 值。

``` r
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
```

额外挑战：通过直接使用 `[[` 来去除匿名函数。

```{r}
# 生成100次t检验结果
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)

# 使用sapply和匿名函数提取每次检验的p值
p_values_anonymous <- sapply(trials, function(test) test$p.value)

# 直接使用[[去除匿名函数，提取每次检验的p值
p_values_direct <- sapply(trials, `[[`, "p.value")

# 输出结果
p_values_anonymous
p_values_direct
```

可以发现，二者结果相同。

```{r}
rm(list = ls()) #清除变量
```

**习题6** 实现一个 `Map()` 和 `vapply()` 的组合，创建一个 `lapply()`
的变体。

```{r}
# 定义一个自定义的并行迭代函数
my_lapply_variant <- function(X, FUN, FUN.VALUE, simplify = FALSE, ...) {
  # 使用 Map() 并行应用函数 FUN 到列表 X 中的每个元素
  results <- Map(function(x) vapply(x, FUN, FUN.VALUE, ...), X)
  
  # 如果 simplify 参数为 TRUE，则将结果简化为数组
  if (simplify) {
    return(simplify2array(results))
  }
  
  # 否则返回结果列表
  results
}
# 定义测试列表
testlist <- list(iris, mtcars, cars)
# 使用 my_lapply_variant 计算每列的均值
lapply(testlist, function(x) vapply(x, mean, numeric(1)))
result <- my_lapply_variant(testlist, mean, numeric(1))
print(result)

```

```{r}
rm(list = ls()) #清除变量
```

#### 练习四

**习题4**

编写一个更快速版本的`chisq.test()`,根据提示，我选择从定义下手：
卡方检验（Chi-squared
test）的数学定义基于以下公式，通常用于检验观察值与期望值之间的差异。

**卡方统计量公式：** 给定一个列联表，其中 $O_{ij}$
表示观测频数，$E_{ij}$ 表示期望频数。卡方统计量 $\chi^2$ 的计算公式为：

$$
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
$$

其中： - $r$ 是列联表的行数。 - $c$ 是列联表的列数。 - $O_{ij}$ 是第 $i$
行第 $j$ 列的观测频数。 - $E_{ij}$ 是第 $i$ 行第 $j$ 列的期望频数。

**期望频数公式：** 期望频数 $E_{ij}$
通常根据列联表的行总和和列总和计算，公式为：

$$
E_{ij} = \frac{(R_i \times C_j)}{N}
$$

其中： - $R_i$ 是第 $i$ 行的总和。 - $C_j$ 是第 $j$ 列的总和。 - $N$
是总样本量，即所有单元格的观测频数之和。

**自由度：** 卡方检验的自由度 $df$ 计算公式为：

$$
df = (r - 1)(c - 1)
$$

**p 值：** p
值是基于卡方统计量和自由度计算的累积概率，表示观察到的卡方统计量值或更极端结果出现的概率。p
值用于决定是否拒绝原假设。

$$
p\text{-value} = P(\chi^2 \geq \chi^2_{\text{observed}} | H_0)
$$

其中： - $\chi^2_{\text{observed}}$ 是计算得到的卡方统计量。 - $H_0$
是原假设，通常假设观察到的数据与期望数据一致。

```{r}
fast_chisq_test <- function(x, y) {
  # 检查输入是否为数值型向量且无缺失值
  if (!is.numeric(x) || !is.numeric(y)) {
    stop("输入必须是数值型向量")
  }
  if (anyNA(x) || anyNA(y)) {
    stop("输入不能包含缺失值")
  }
  
  # 创建列联表
  m <- rbind(x, y)
  margin1 <- rowSums(m)  # 计算行总和
  margin2 <- colSums(m)  # 计算列总和
  n <- sum(m)  # 总和
  me <- tcrossprod(margin1, margin2) / n  # 计算期望频数

  # 计算卡方统计量
  x_stat <- sum((m - me)^2 / me)

  # 计算自由度
  dof <- (length(margin1) - 1) * (length(margin2) - 1)

  # 计算p值
  p <- pchisq(x_stat, df = dof, lower.tail = FALSE)

  # 返回结果
  return(list(x_stat = x_stat, df = dof, `p-value` = p))
}

# 使用示例
a <- 12:16
b <- c(11, 13, 15, 17, 19)

# 使用我们改进后的 fast_chisq_test
fast_chisq_result <- fast_chisq_test(a, b)
print(fast_chisq_result)
# 使用原本的 fast_chisq_test
m_test <- cbind(a, b)
chisq_result <- chisq.test(m_test)
print(chisq_result)
```

可以发现我们编写的`fast_chisq_test`与`chisq.test`得到的结果相同。

```{r}
rm(list = ls()) #清除变量
```

**习题5**

为两个没有缺失值的整数向量输入，制作一个更快速的 `table()`
版本，并使用它来加速卡方检验。

首先是编写一个更快速的 `table()` 版本：

```{r}
fast_table <- function(x, y) {
  # 确保 x 和 y 是整数向量
  if (!is.integer(x)) x <- as.integer(x)
  if (!is.integer(y)) y <- as.integer(y)
  
  # 获取 x 和 y 的所有唯一值
  levels_x <- unique(x)
  levels_y <- unique(y)
  
  # 创建一个空的矩阵
  result_matrix <- matrix(0, nrow = length(levels_x), ncol = length(levels_y))
  
  # 为每个值找到索引位置
  x_index <- match(x, levels_x)
  y_index <- match(y, levels_y)
  
  # 直接通过索引填充矩阵
  result_matrix[cbind(x_index, y_index)] <- result_matrix[cbind(x_index, y_index)] + 1
  
  # 将矩阵命名
  dimnames(result_matrix) <- list(levels_x, levels_y)
  
  return(result_matrix)
}

# 使用示例
x <- 12:16
y <- c(11, 13, 15, 17, 19)

# 调用快速版的 table()
fast_table_result <- fast_table(x, y)
print(fast_table_result)
table(x,y)

```

将加速后的table 函数用于自定义卡方检验：

```{r}
# 定义加速版的 table 函数
fast_table <- function(x, y) {
  # 确保 x 和 y 是整数向量
  if (!is.integer(x)) x <- as.integer(x)
  if (!is.integer(y)) y <- as.integer(y)
  
  # 获取 x 和 y 的所有唯一值
  levels_x <- unique(x)
  levels_y <- unique(y)
  
  # 初始化空矩阵
  result_matrix <- matrix(0, nrow = length(levels_x), ncol = length(levels_y))
  
  # 创建一个元素的索引映射，方便查找
  x_index <- match(x, levels_x)
  y_index <- match(y, levels_y)
  
  # 填充频数矩阵
  for (i in 1:length(x)) {
    result_matrix[x_index[i], y_index[i]] <- result_matrix[x_index[i], y_index[i]] + 1
  }
  
  # 将矩阵命名
  dimnames(result_matrix) <- list(levels_x, levels_y)
  
  return(result_matrix)
}

# 自定义的卡方检验
fast_chisq_test <- function(x, y) {
  # 使用加速版的 table 创建列联表
  tbl <- fast_table(x, y)
  
  # 计算观测频数
  O <- as.vector(tbl)
  
  # 计算期望频数
  margin1 <- rowSums(tbl)
  margin2 <- colSums(tbl)
  n <- sum(tbl)
  E <- outer(margin1, margin2, FUN = "*") / n
  
  # 计算卡方统计量
  x_stat <- sum((O - E)^2 / E)
  
  # 计算自由度
  dof <- (nrow(tbl) - 1) * (ncol(tbl) - 1)
  
  # 计算p值
  p <- pchisq(x_stat, df = dof, lower.tail = FALSE)
  
  # 返回结果
  return(list(x_stat = x_stat, df = dof, `p-value` = p))
}

# 使用示例
a <- 12:16
b <- c(11, 13, 15, 17, 19)

# 使用加速版的卡方检验
fast_chisq_result <- fast_chisq_test(a, b)
print(fast_chisq_result)
chisq_result <- chisq.test(a, b)
print(chisq_result)
```

可以发现二者结果相同。但有一个值得注意的地方，**习题4**部分中，我定义的fast_chisq_test并不包含table函数部分。但在**习题5**中，为了使用`table()`函数，我们实际将输入的`m_test <- cbind(a, b)`改为了直接输入`(a,b)`，这会使得`chisq.test`函数自动构建列联表
table(x,y)，然后计算卡方检验，从而使用上`table()`函数。

```{r}
rm(list = ls()) #清除变量
```

### Homework-9

```{r}
options(warn = -1)  # 关闭所有警告(因为使用的R版本较低，调用一些R包时会显示警告)
rm(list = ls()) #清除变量
```

##### 练习1

**题目：** 考虑一个二元密度函数：

$$
f(x, y) \propto \binom{n}{x} y^{x + a - 1} (1 - y)^{n - x + b - 1}, \quad x = 0, 1, \dots, n, \quad 0 \leq y \leq 1.
$$

可以证明，对于固定的 $a$、$b$、$n$，其条件分布分别为二项分布
$\text{Binomial}(n, y)$ 和贝塔分布
$\text{Beta}(x + a, n - x + b)$。使用吉布斯采样器生成以目标联合密度
$f(x, y)$ 为目标的链。

**注意事项：**这里我编写了一个名为`gibbs_sampler`的cpp文件放在R包中

```{r}
library(Rcpp)
library(SA24204158)

# 设定参数
n_iter <- 10000  # 迭代次数
n <- 10          # Binomial 的参数
a <- 2           # Beta 分布的第一个参数
b <- 2           # Beta 分布的第二个参数

# 调用 Gibbs 采样器
set.seed(123)
samples <- gibbs_sampler(n_iter, n, a, b)

# 转换为数据框，方便可视化
samples_df <- as.data.frame(samples)
colnames(samples_df) <- c("x", "y")

# 可视化
library(ggplot2)
ggplot(samples_df, aes(x = x, y = y)) +
  geom_point(alpha = 0.5) +
  labs(title = "Gibbs 采样结果", x = "x", y = "y")
```

#### 练习2

**题目：**用`qqplot`对比分别用Rcpp和R代码生成的分布

```{r}
# 设置参数
a <- 2
b <- 2
n <- 10
num_iterations <- 5000

# 使用 Rcpp 的 Gibbs 采样器生成样本
set.seed(123)
cpp_samples <- gibbs_sampler(num_iterations, n, a, b)

# 使用 R 实现的 Gibbs 采样器生成样本
set.seed(123)
samples_x_r <- numeric(num_iterations)
samples_y_r <- numeric(num_iterations)
samples_x_r[1] <- 5
samples_y_r[1] <- 0.5

for (i in 2:num_iterations) {
  samples_x_r[i] <- rbinom(1, n, samples_y_r[i - 1])
  samples_y_r[i] <- rbeta(1, samples_x_r[i] + a, n - samples_x_r[i] + b)
}

# 使用 qqplot 比较 x 和 y 的分布
par(mfrow = c(1, 2))
qqplot(cpp_samples[, 1], samples_x_r, main = "Rcpp vs R (x)", xlab = "Rcpp x", ylab = "R x")
abline(0, 1, col = "red", lwd = 2)

qqplot(cpp_samples[, 2], samples_y_r, main = "Rcpp vs R (y)", xlab = "Rcpp y", ylab = "R y")
abline(0, 1, col = "red", lwd = 2)

```

从`qqplot`的对比图可以发现，使用Rcpp生成的样本与使用R代码生成的样本，分位数对应的非常好，二者服从相同分布。

#### 练习3

**题目：**使用 `microbenchmark` 比较 R 实现和 Rcpp 实现的运行时间。

这里我们基于20次重复运算来对比结果。

```{r}
library(microbenchmark)

# 定义 R 实现的 Gibbs 采样器为一个函数
gibbs_r <- function(num_iterations, n, a, b) {
  samples_x <- numeric(num_iterations)
  samples_y <- numeric(num_iterations)
  samples_x[1] <- 5
  samples_y[1] <- 0.5
  for (i in 2:num_iterations) {
    samples_x[i] <- rbinom(1, n, samples_y[i - 1])
    samples_y[i] <- rbeta(1, samples_x[i] + a, n - samples_x[i] + b)
  }
  list(x = samples_x, y = samples_y)
}

# 比较时间
benchmark_result <- microbenchmark(
  Rcpp = gibbs_sampler(num_iterations, n, a, b),
  R = gibbs_r(num_iterations, n, a, b),
  times = 20
)

print(benchmark_result)
```

从`microbenchmark`的结果可以看出，Rcpp的运算效率比直接使用R高，对比均值发现Rcpp运算效率比R高大概20倍，这体现了C++的优势。

```{r}
rm(list = ls()) #清除变量
```

### 总结

这个学期学习了统计计算这门课程，在课上收获很多，最重要的一点是学会了编写自己的R包，在此感谢张洪老师和王齐羽助教的细心解答！
